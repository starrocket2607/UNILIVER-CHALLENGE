{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dove Hair"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Deciding on the product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use iphone, why iphone? idk Some justification please xd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Rough Overview and workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image of the workflow, from hubert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.The Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Installing libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below are several libraries that were used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install amazon_search_results_scraper\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install selenium\n",
    "# !pip install clean-text\n",
    "# !pip install langdetect\n",
    "# !pip install google-api-python-client\n",
    "# !pip install youtube-transcript-api\n",
    "# !pip install praw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating folders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these functions were run at different times. Some simultaneously, some out of order, some over and over while testing... \n",
    "\n",
    "In order to prevent the data from becoming too messy, a folder to store the output of the various functions was created.\n",
    "\n",
    "Data: The main folder, stores the initial data from webscrapping various websites\n",
    "\n",
    "First_pass: Stores the data after they have been labelled in the first zero-shot classifier for the purposes of data cleaning\n",
    "\n",
    "Second_pass: Stores the data after they have pass through the second classifier and have been labelled for the sake of classification\n",
    "\n",
    "Final: The cleaned data from the different platforms compiled together and then are labelled with sentiment analysis and are then stored here\n",
    "\n",
    "The function <b> create_directories() </b> doesn't take any input and is used to create the directories used to store the information and various csvs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def create_directories():\n",
    "    try:\n",
    "        os.makedirs(\"Data\")\n",
    "        print(\"Base directory created\")\n",
    "    except:\n",
    "        print(\"base directory: Data exists\")\n",
    "    folders = ['Final', 'First_pass', 'Second_pass']\n",
    "    for i in folders:\n",
    "        path = 'Data'\n",
    "        try:\n",
    "            os.makedirs(os.path.join(path, i))\n",
    "        except:\n",
    "            print(\"Folder \" + i + \" exists.\")\n",
    "\n",
    "create_directories()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Pre-analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific product is the only input needed for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'Dove Shampoo'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Forming the categories to classify comments into"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, we need to know more about the existing product we are trying to improve. \n",
    "\n",
    "This will allow us to create cactegories specific to the product we are trying to analyse so that we can then classify the comments that are scrapped properly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Using openAI to get the strengths, flaws and competitors of the product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that was done was to use the openai library to find the strengths and the flaws of the product. We use specific prompts to ensure that the response is in a certain format that can be converted to a python dictionary.\n",
    "\n",
    "The Competitors was also collected for use in data cleaning in the subsequent steps.\n",
    "\n",
    "The function <b> ask_gpt(prompt) </b> is a general function that takes a prompt and sends it to the openai website before returning an answer generated in a string\n",
    "\n",
    "This function is subsequently used in the functions <b> get_design_strength(keyword) </b>, <b> get_design_flaws(keyword) </b>, and <b> get_competitors(keyword) </b>. They each take the product ('iphone')  and then return a string of the dictionary of the response to the prompt created in the function.\n",
    "\n",
    "It should be noted that this function was run with the particular keyword, iphone 11 instead iphone. But as all the webscrapping was already done, there would not be enough time to rescrape all the websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\", prompt=prompt, max_tokens=1024, n=1, stop=None, temperature=0.1\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "\n",
    "def get_design_strength(keyword):\n",
    "    return ask_gpt(\"Give me the design strengths of the\" + keyword + \"in a python dictionary format\")\n",
    "\n",
    "def get_design_flaws(keyword):\n",
    "    return ask_gpt(\"Give me the design flaws of the\" + keyword + \"in a python dictionary format\")\n",
    "\n",
    "def get_competitors(keyword):\n",
    "    return ask_gpt(\"Give me the competitors of the\" + keyword + \"in a python dictionary format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0f6315c45ebb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresponse2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_design_flaws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dove Hair Therapy Breakage Remedy Shampoo with Bio-Cellular Complex & Vitamin C\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresponse3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_design_strength\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dove Hair Therapy Breakage Remedy Shampoo with Bio-Cellular Complex & Vitamin C\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresponse4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_competitors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dove Hair Therapy Breakage Remedy Shampoo with Bio-Cellular Complex & Vitamin C\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-79c9ddf597fe>\u001b[0m in \u001b[0;36mget_design_flaws\u001b[1;34m(keyword)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_design_flaws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mask_gpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Give me the design flaws of the\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"in a python dictionary format\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_competitors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-79c9ddf597fe>\u001b[0m in \u001b[0;36mask_gpt\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mask_gpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     response = openai.Completion.create(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"text-davinci-003\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\api_resources\\completion.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    151\u001b[0m         )\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         )\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m             return (\n\u001b[1;32m--> 619\u001b[1;33m                 self._interpret_response_line(\n\u001b[0m\u001b[0;32m    620\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    680\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m             raise self.handle_error_response(\n\u001b[0m\u001b[0;32m    683\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "response2 = get_design_flaws(\"Dove Hair Therapy Breakage Remedy Shampoo with Bio-Cellular Complex & Vitamin C\")\n",
    "response3 = get_design_strength(\"Dove Hair Therapy Breakage Remedy Shampoo with Bio-Cellular Complex & Vitamin C\")\n",
    "response4 = get_competitors(\"Dove Hair Therapy Breakage Remedy Shampoo with Bio-Cellular Complex & Vitamin C\")\n",
    "print(response2)\n",
    "print(response3)\n",
    "print(response4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from the openAI is currently a string. So the ast library is used to convert it from a string into a python dictionary.\n",
    "\n",
    "The function <b> convert_string_dict(string) </b> will then take the string returned from openai and convert it into a dictionary. Do note that the string has to be in a specific format to return a python dictionary properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert_string_to_dict(string):\n",
    "    try:\n",
    "        index = string.index(\"=\")\n",
    "        string = string[index+1:].strip()\n",
    "    except:\n",
    "        #no =\n",
    "        pass\n",
    "    return ast.literal_eval(string)\n",
    "\n",
    "design_flaws =(convert_string_to_dict(response2))\n",
    "design_strengths = (convert_string_to_dict(response3))\n",
    "competitors = (convert_string_to_dict(response4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Scrape google shopping to get categories "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will scrape through the google shopping website. The google shopping website sorts reviews into distinct categories, which can be collected and used as potential categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, <b> googleshopping_get_id </b> and <b> googleshopping_get_keywords </b> were used to scrape the google shopping websites for their keywords. \n",
    "\n",
    "The function<b> googleshopping_get_id </b> is used first, it takes a list of search terms and will use that to find pages to scrape, returning a list of product_ids to look at.\n",
    "\n",
    "The function <b> googleshopping_get_keywords </b> will then take the product_ids and the search terms in order to scrape the reviews from the various pages and return all the keywords stored in a df.\n",
    "\n",
    "The function <b> save_data(data, file_name) </b> will then take the dataframe returned from the <b> googleshopping_get_keywords </b> and will then store it in the <b> Data </b> folder with the file_name, <b> google_shopping_scraped.csv </b> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_data(data, file_name):\n",
    "    try:                                            # Create directory named after search terms\n",
    "        os.makedirs(\"Data\")\n",
    "        print(\"Directory created\")\n",
    "\n",
    "    except FileExistsError:\n",
    "        print(\"Directory exists\")\n",
    "\n",
    "    data.to_csv(\"Data/%s.csv\" % (file_name))\n",
    "\n",
    "def insearch_result(search_term, title):\n",
    "    title = title.lower()\n",
    "    for keywords in search_term:\n",
    "        if keywords not in title:\n",
    "            # print(title)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def googleshopping_get_id(search_terms):\n",
    "    base_url = \"https://shopping.google.com/\"\n",
    "\n",
    "    # initialising chrome and going to the url\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # finding search bar and searching for search_terms\n",
    "    search = driver.find_element(\"xpath\", \"//*[@id='REsRA']\")\n",
    "    # print(\"The input Element is: \", search)\n",
    "    search.send_keys(\" \".join(search_terms))\n",
    "    search.send_keys(Keys.RETURN)\n",
    "\n",
    "    # pausing to let the page load and scrape the page html\n",
    "    time.sleep(5)\n",
    "    page = driver.page_source\n",
    "\n",
    "    # close the driver\n",
    "    time.sleep(2)\n",
    "    driver.close()\n",
    "\n",
    "    # parse the page\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    product_id = []\n",
    "    for links in soup.find_all('a', href=True):\n",
    "        if \"/shopping/product\" in links['href']:\n",
    "            end_index = links['href'].index(\"?\")\n",
    "            id = links['href'][18:end_index]\n",
    "\n",
    "            if id not in product_id:\n",
    "                product_id.append(id)\n",
    "\n",
    "    return product_id\n",
    "\n",
    "\n",
    "def googleshopping_get_keywords(productID, search_terms):\n",
    "    # Require a list of ProductID\n",
    "    base_url = \"https://www.google.com/shopping/product/\"\n",
    "    df = pd.DataFrame(columns=[\"Title\", \"Number of Reviews\",\n",
    "                      \"Keywords\", \"Percentage\", \"Positive/Negative\"])\n",
    "\n",
    "    for i in range(len(productID)):\n",
    "\n",
    "        if \"/offers\" in productID[i]:\n",
    "            index = productID[i].index(\"/offers\")\n",
    "            productID[i] = productID[i][:index]\n",
    "\n",
    "        url = base_url + productID[i] + \"/reviews\"\n",
    "        # print(url)\n",
    "\n",
    "        driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        page = driver.page_source\n",
    "        time.sleep(2)\n",
    "        driver.close()\n",
    "\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        title = soup.find(\"div\", {\"class\": \"f0t7kf\"}).text\n",
    "        anyreviews = soup.find(\"div\", {\"class\": \"rktlcd\"})\n",
    "\n",
    "        if anyreviews == None:\n",
    "            # print(\"No reviews found\")\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            intitle = insearch_result(search_terms, title)\n",
    "\n",
    "            if intitle:\n",
    "                for span in soup.find_all(\"span\", \"QIrs8\"):\n",
    "                    text = span.text\n",
    "                    # print(text)\n",
    "                    if text == \"Select to view all reviews\":\n",
    "                        pass\n",
    "\n",
    "                    else:\n",
    "                        # print(text)\n",
    "                        reviews_index = text.index(\"r\")\n",
    "                        about_index = text.index(\"about\")\n",
    "                        full_stop_index = text.index(\".\")\n",
    "                        percentage_index = text.index(\"%\")\n",
    "\n",
    "                        num_of_reviews = text[4: reviews_index].strip()\n",
    "                        keywords = text[about_index +\n",
    "                                        len(\"about\")+1: full_stop_index]\n",
    "                        percentage = text[full_stop_index+2: percentage_index]\n",
    "                        positivenegative = text[-9:-1]\n",
    "                        # print(num_of_reviews, keywords,\n",
    "                        #       percentage, positivenegative)\n",
    "                        df = df.append({\"Title\": title, \"Number of Reviews\": num_of_reviews, \"Keywords\": keywords,\n",
    "                                       \"Percentage\": percentage, \"Positive/Negative\": positivenegative}, ignore_index=True)\n",
    "                        # print(df)\n",
    "    return df\n",
    "\n",
    "search_terms = ['dove hair']\n",
    "product_ids = googleshopping_get_id(search_terms)\n",
    "data = googleshopping_get_keywords(product_ids, search_terms)\n",
    "save_data(data, \"google_shopping_scraped\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv that was extracted can be seen in the Data folder, with the name google_shopping_scraped.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then extract the keywords of from the csvs, making sure to exclude identical tags.\n",
    "\n",
    "The function <b> extract_keywords_from_google_shopping_csv(file_location) </b> takes the location where the google shopping csv is stored and will then extract the keywords and return it as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def extract_keywords_from_google_shopping_csv(file_location):\n",
    "    keywords = []\n",
    "    goolge_pd = pd.read_csv(file_location)\n",
    "    for index, row in goolge_pd.iterrows():\n",
    "        keyword = row[3]\n",
    "        if keyword not in keywords:\n",
    "            keywords.append(keyword)\n",
    "    return keywords\n",
    "\n",
    "google_keywords = (extract_keywords_from_google_shopping_csv(\"Data\\google_shopping_scraped.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Combine all keywords found (removing repeats) and use siamese network to remove similar categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we will then combine the keywords found together. The first step is to create a giant list of all the keywords.\n",
    "\n",
    "The function <b> combine_keywords(strengths: list,flaws:list, google_keywords: list) </b> then takes lists of the strengths, flaws and google_keywords generated from the previous functions and returns a list of the compiled keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_keywords(strengths: list,flaws:list, google_keywords: list):\n",
    "    keywords = []\n",
    "    for strength in strengths:\n",
    "        if strength not in keywords:\n",
    "            keywords.append(strength).lower()\n",
    "    for flaw in flaws:\n",
    "        if flaw not in keywords:\n",
    "            keywords.append(flaw).lower()\n",
    "    for goolge in google_keywords:\n",
    "        if goolge not in keywords:\n",
    "            keywords.append(goolge).lower()\n",
    "    return keywords\n",
    "\n",
    "All_keywords = (combine_keywords(design_flaws.keys(),design_strengths.keys(),google_keywords))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the ouptut from combine keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_keywords = [\"Hair type\", \"Ingredients\", \"Brand Reputation\", \"Gentle formula\", \"Moisturizing properties\", \"Product claims\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what was observed is that certain keywords were quite similar. A siamese network was then used to identify keywords that were similar and remove them. \n",
    "\n",
    "<b> This code was run on collab. </b>\n",
    "\n",
    "The function <b> siamese_analysis(All_keywords) </b> takes the list of all the keywords from the previous steps and returns a list of keywords that has been cleaned with a siamese analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go to Collab\n",
    "from scipy import spatial\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "def siamese_analysis(All_keywords):\n",
    "  model = api.load(\"glove-wiki-gigaword-50\") #choose from multiple models https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "  keywords = All_keywords\n",
    "  def preprocess(s):\n",
    "    return [i.lower() for i in s.split()]\n",
    "\n",
    "  def get_vector(s):\n",
    "    return np.sum(np.array([model[i] for i in preprocess(s)]), axis=0)\n",
    "\n",
    "  vector_data = []\n",
    "  for keyword in keywords:\n",
    "    print(keyword)\n",
    "    \n",
    "    value = get_vector(keyword)\n",
    "    vector_data.append(value)\n",
    "    \n",
    "\n",
    "  cleaned_keywords = []\n",
    "\n",
    "  for i in range(len(vector_data)):\n",
    "    for j in range(i, len(vector_data)):\n",
    "      similarity = 1 - spatial.distance.cosine(vector_data[i], vector_data[j])\n",
    "      if similarity < 0.2:\n",
    "        cleaned_keywords.append(keywords[i])\n",
    "        cleaned_keywords.append(keywords[j])\n",
    "        # print(keywords[i], keywords[j])\n",
    "\n",
    "\n",
    "  cleaned_keywords = list(set(cleaned_keywords))\n",
    "  return cleaned_keywords\n",
    "\n",
    "keywords = All_keywords\n",
    "cleaned_keywords = siamese_analysis(keywords)\n",
    "# print(cleaned_keywords)\n",
    "print(len(keywords), len(cleaned_keywords))\n",
    "print(cleaned_keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the cleaned keywords from the run on the platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_keywords = [\"Hair type\", \"Ingredients\", \"Brand Reputation\", \"Gentle formula\", \"Moisturizing properties\", \"Product claims\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Remove categories that are substring of other categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were still a few keyword categories that were substrings of other keywords like battery life and long battery life.\n",
    "\n",
    "The function, <b> create_keywords </b>, takes the cleaned keywords from the previous step and removes the shorter string in the case that one string was the substring of another string. It then returns the list of the final keywords which will be stored in the variable <b> keywords </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keywords(cleaned_keywords):\n",
    "    cleaned_keywords.sort(key=len, reverse=False)\n",
    "\n",
    "    for i in range(len(cleaned_keywords)):\n",
    "        cleaned_keywords[i] = cleaned_keywords[i].lower()\n",
    "    print(cleaned_keywords)\n",
    "\n",
    "    # loop through each string in the list\n",
    "    for i in range(len(cleaned_keywords)):\n",
    "        # compare the string with all subsequent strings in the list\n",
    "        for j in range(i+1, len(cleaned_keywords)):\n",
    "            if cleaned_keywords[i] in cleaned_keywords[j]:\n",
    "                # if the current string is a substring of another string, remove it from the list\n",
    "                cleaned_keywords.pop(i)\n",
    "                break\n",
    "        else:\n",
    "            # if the current string is not a substring of any subsequent strings, move on to the next string\n",
    "            continue\n",
    "        # if the current string was removed from the list, adjust the index accordingly\n",
    "        i -= 1\n",
    "    return cleaned_keywords\n",
    "\n",
    "keywords = create_keywords(cleaned_keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the keywords that were generated in the initial run of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Hair type\", \"Ingredients\", \"Brand Reputation\", \"Gentle formula\", \"Moisturizing properties\", \"Product claims\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the input of the product, we are able to fully automate the process of getting the following outputs:\n",
    "\n",
    "<b> google_shopping_scraped.csv </b> is also saved in the <b> Data </b> folder with categories and number of reviews under each category.\n",
    "\n",
    "<b>product </b>: A string of the name of the product that will be the focus of the study. For the sake of this report, this product will be 'iphone'\n",
    "\n",
    "<b>keywords </b>: A list of keywords that can be used for classification using zero-shot or other text analysis modesl\n",
    "\n",
    "<b>competitors </b>: A dictionary of competing products and companies that can be used for data screening\n",
    "\n",
    "These are the following outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Hair type\", \"Ingredients\", \"Brand Reputation\", \"Gentle formula\", \"Moisturizing properties\", \"Product claims\"]\n",
    "\n",
    "competitors = {\n",
    "    \"Pantene\": \"Pantene hair\",\n",
    "    \"L' Oreal Paris\": \"L'Oreal Paris hair\",\n",
    "    \"Head & Shoulders\": \"Head & Shoulders hair\",\n",
    "    \"Clear\": \"Clear hair\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Big Data collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to step 3, we are still collecting data and all we need is the product name (for the general websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'Dove hair'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Choice of websites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wide range of websites were chosen to collect the dataset that would be used to analyse the ways to improve the product.\n",
    "\n",
    "Given that the product would usually be unknown, a few generic websites were chosen such that any product could appear on them. This led to the following websites being choosen:\n",
    "\n",
    "<b>Amazon </b>: A popular e-commerce websites that is used for a wide range of products. It should be noted that amazon does have an anti-scraping policy. However this project was done for education purposes not for the sake of monetization and was done solely as a learning experience and proof of concept.\n",
    "\n",
    "<b>Reddit </b>: Reddit is used as a wide range of topics are discussed on the site and is one of the more popular forum sites. \n",
    "\n",
    "<b>Youtube </b>: A very popular video playback site, there are a range of prodect reviews on the site, which will be prime for comment scrapping.\n",
    "\n",
    "\n",
    "While doing the report, websites that were more related to the product were also scrapped and considered. It should be noted that if the entire process is to be automated, the database from these websites should be excluded.\n",
    "\n",
    "<b>Hardware Forum zone </b>: This is a tech forum used by Singaporeans. They have a page dedicated to discussing the iphone and hence was selected for analysis.\n",
    "\n",
    "<b> Apple Insider </b>: This page is a page that releases articles on apple products. It was also scrapped initially, however it was cut from the final version as it was too niche for the exam of the iphone chosen and the user base was likely to be biased as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Scrapping the websites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Amazon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the amazon_search_results_scraper API, we are able to find the product pages and then use selenium to collect the comments under the product pages found. \n",
    "\n",
    "The following function, <b>search_amazon(keyword, exceptions): </b>, takes the input of the product ('iphone') as the keyword, and will find the product pages. The function then uses selenium to go to the first page of the reviews and then scrape the comments from it. It should be noted that an exceptions variable was needed to optimise the search. However the creation of the exceptions keyword was not automated and inputed by a user. The function would then return a dataframe of the comments scraped.\n",
    "\n",
    "\n",
    "The function <b> save_data(data, file_name) </b> will then take the dataframe returned from  <b> search_amazon(keyword, exceptions) </b> and will then store it in the <b> Data </b> folder with the file_name, <b>amazon_reviews.csv </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_search_results_scraper import *\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import random\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "def checkvalid_us(main_keyword, i):\n",
    "    try:\n",
    "        title = i['link']\n",
    "        if main_keyword in title:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def create_link_to_crpage_us(link):\n",
    "    new_link = link.replace(\"dp\", \"product-reviews\")\n",
    "    new_link, chop, chop_liver = new_link.partition(\"ref=\")\n",
    "    new_link = new_link + \\\n",
    "        'ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&pageNumber=1&sortBy=recent'\n",
    "    return new_link\n",
    "\n",
    "def amazon_create_different_page_links(link, number_of_pages_iteration=5):\n",
    "    links = []\n",
    "    links.append(link)\n",
    "    for i in range(2, (number_of_pages_iteration + 1)):\n",
    "        linked = links[-1]\n",
    "        new_link = linked.replace(\n",
    "            'pageNumber=' + (str(i-1)), 'pageNumber=' + (str(i)))\n",
    "        links.append(new_link)\n",
    "    return links\n",
    "    \n",
    "def search_amazon(keyword, exceptions):\n",
    "    amazon.open(\"https://www.amazon.com/\")\n",
    "    main_keyword = keyword\n",
    "    title_exceptions = exceptions\n",
    "    amazon.search(keyword=main_keyword)\n",
    "\n",
    "    response = amazon.search_results()\n",
    "    search_results = response['body']\n",
    "\n",
    "    pages = []\n",
    "    for i in search_results:\n",
    "        if checkvalid_us(main_keyword, i):\n",
    "            pages.append(i['link'])\n",
    "        else:\n",
    "            pass\n",
    "            # print('no valid link or no valid_title')\n",
    "\n",
    "    # print(create_link_to_crpage_us(pages[1]))\n",
    "\n",
    "    custom_review_pages = []\n",
    "    for i in pages:\n",
    "        to_add = create_link_to_crpage_us(i)\n",
    "        custom_review_pages.append(to_add)\n",
    "    \n",
    "    #links stored in custom_review_pages\n",
    "    df = pd.DataFrame(columns=[\"Title\" , \"Link\", \"Stars\", \"Comments\"])\n",
    "    list_links = []\n",
    "    for link in custom_review_pages:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\n",
    "        'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(link)\n",
    "        htmlsource = driver.page_source\n",
    "        soup = BeautifulSoup(htmlsource, 'html.parser')\n",
    "        driver.quit()\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            title = soup.find('h1')\n",
    "            title = (title.findChildren('span')[0].text.strip())\n",
    "            review_section = soup.find('div', {'id': \"cm_cr-review_list\"})\n",
    "            # find all reviews\n",
    "            reviews = review_section.find_all('div', {'class': \"review\"})\n",
    "            # for each review\n",
    "            for i in reviews:\n",
    "                # collect stars\n",
    "                stars = i.find('i', {'class': 'review-rating'})\n",
    "                stared = (stars.findChildren('span')[0]).text.strip()\n",
    "                # collect review\n",
    "                review = i.find(\n",
    "                    'a', {'class': 'a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold'})\n",
    "                review = (review.findChildren(\n",
    "                    'span', recursive=True)[0]).text.strip()\n",
    "                df.loc[len(df)]  = {\"Title\": title, \"Link\": link, \"Stars\": str(stared), \"Comments\":str(review)}  \n",
    "        except:\n",
    "            print(soup)\n",
    "        time.sleep(10)\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_reviews =search_amazon('dove hair', ['Case', 'case'])\n",
    "save_data(df_amazon_reviews, \"amazon_reviews\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Reddit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using praw, reddits own built in api, one can scrape the website for comments about a certain topic.\n",
    "\n",
    "The following function, <b> combined_reddit(search_term: str, post_limit=10): </b>, takes the input of the product ('iphone') as the search_term, and will find the pages related to it and collect the comments from the top pages. It then returns a dataframe of the information.\n",
    "\n",
    "The function <b> save_data(data, file_name) </b> will then take the dataframe returned from  <b> combined_reddit(search_term: str, post_limit=10): </b> and will then store it in the <b> Data </b> folder with the file_name, <b>reddit_scrapped.csv </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import re\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "\n",
    "def scrape_reddit(search_term:str, post_limit=10):\n",
    "    df = pd.DataFrame(columns=['Posts', 'Comments'])\n",
    "    reddit = praw.Reddit(client_id='sQePkNsCdxJgOehVWkLa6A',\n",
    "                         client_secret='52ZjTdPwfQLP_RuXKcnDPe2Vs2Myxg',\n",
    "                         user_agent='<console:HAPPY:1.0')\n",
    "    subreddit = reddit.subreddit(search_term)\n",
    "    for submission in subreddit.hot(limit=post_limit):\n",
    "        for comment in submission.comments:\n",
    "            if hasattr(comment,'body'):\n",
    "                index = df.shape[0]\n",
    "                df.loc[index] = [submission.title, comment.body]\n",
    "    return df\n",
    "\n",
    "def cleanup_reddit(uncleaned_frame):\n",
    "    df = pd.DataFrame(columns=['Posts', 'Comments'])\n",
    "    uncleaned_frame.reset_index()\n",
    "    for index, row in uncleaned_frame.iterrows():\n",
    "        Post, Comment = row['Posts'], row['Comments']\n",
    "        if Comment == \"[deleted]\":\n",
    "            continue\n",
    "        subComments = []\n",
    "        commented = Comment.split(\". \")\n",
    "        for x in commented:\n",
    "            subComments.append(x)\n",
    "        for i in subComments:\n",
    "            commentina = re.split(\"[?:!]\", i)\n",
    "            for j in commentina:\n",
    "                j = clean(j, no_emoji=True)\n",
    "                if j == \"\":\n",
    "                    continue\n",
    "                index = df.shape[0]\n",
    "                df.loc[index] = [Post, j]\n",
    "    return df\n",
    "\n",
    "\n",
    "def combined_reddit(search_term: str, post_limit=10):\n",
    "    df = scrape_reddit(search_term,post_limit)\n",
    "    output = cleanup_reddit(df)\n",
    "    return output\n",
    "        \n",
    "dataframe_reddit = combined_reddit(\"dove hair\")\n",
    "save_data(dataframe_reddit, \"reddit_scrapped\")        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Youtube"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using youtube's API, we are able to find the top videos of a certain search and scrape the comments from them.\n",
    "\n",
    "The following function, <b> youtube_search(search_terms, 5) </b>, takes the input of the product ('iphone') and the keyword 'review' as the search_term, and will find the 5 videos related to it and collect the video ids of the top pages. It then returns a list of the video ids.\n",
    "\n",
    "The function, <b> youtube_comments(vid_id) </b> then takes the list of video ids and scrapes the comments from the videos and puts it into the a dataframe.\n",
    "\n",
    "The function <b> save_data(data, file_name) </b> will then take the dataframe returned from  <b>youtube_comments(vid_id) </b> and will then store it in the <b> Data </b> folder with the file_name, <b>youtube comments.csv </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import pandas as pd\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "from cleantext import clean\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"Insert your own key here\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "\n",
    "def youtube_search(search_term, maxresults):  # returns video ID\n",
    "    # assuming that maxresults is always <=50\n",
    "    vidID = []\n",
    "\n",
    "    # print(\"Searching for videos\")\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        q=\" \".join(search_term),\n",
    "        part=\"id\",\n",
    "        type=\"video\",\n",
    "        maxResults=maxresults)\n",
    "\n",
    "    search_response = request.execute()\n",
    "\n",
    "    for i in range(maxresults):\n",
    "        videoID = search_response['items'][i]['id'][\"videoId\"]\n",
    "        # videoLinks = \"https://www.youtube.com/watch?v=\" + videoID\n",
    "        vidID.append(videoID)\n",
    "\n",
    "    return vidID\n",
    "\n",
    "\n",
    "# using video ID, get all the comments and likes to put into dataframe\n",
    "def youtube_comments(vidID):\n",
    "    df = pd.DataFrame(columns=['title', 'Comments', 'likes'])\n",
    "    for i in range(len(vidID)):\n",
    "\n",
    "        title, video_response = get_title(vidID[i])\n",
    "\n",
    "        try:  # use try/except to check if comments exists\n",
    "            comment_count = video_response['items'][0]['statistics']['commentCount']\n",
    "            # print(\"Video-\", title, \"-- Comment count: \", comment_count)\n",
    "\n",
    "            request_comment = youtube.commentThreads().list(\n",
    "                part=\"snippet, replies\",\n",
    "                videoId=vidID[i])\n",
    "\n",
    "            comment_response = request_comment.execute()\n",
    "            df = get_all_comments(comment_response, title, df)\n",
    "            test = comment_response.get(\"nextPageToken\", \"nil\")\n",
    "\n",
    "            while test != 'nil':  # load next page of comments\n",
    "                next_page_ = comment_response.get('nextPageToken')\n",
    "                request = youtube.commentThreads().list(  # new request for next pag\n",
    "                    part=\"snippet,replies\",\n",
    "                    pageToken=next_page_,\n",
    "                    videoId=vidID[i]\n",
    "                )\n",
    "                comment_response = request.execute()\n",
    "\n",
    "                df = get_all_comments(comment_response, title, df)\n",
    "                test = comment_response.get('nextPageToken', 'nil')\n",
    "\n",
    "        except:\n",
    "            print(\"Video\", i + 1, \"-\", title,\n",
    "                  \"-- Comments are turned off, ignoring video\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_title(vid_id):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet, statistics\",\n",
    "        id=vid_id)\n",
    "\n",
    "    video_response = request.execute()\n",
    "    title = video_response['items'][0]['snippet']['title']\n",
    "    return title, video_response\n",
    "\n",
    "\n",
    "def get_all_comments(response, title, df):\n",
    "    for comment in response['items']:\n",
    "        comment_text = comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comment_text = clean(comment_text, no_emoji=True)\n",
    "        likes_count = comment['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "        # print(comment_text, likes_count)\n",
    "        if 'replies' in comment.keys():\n",
    "            for reply in comment['replies']['comments']:\n",
    "                rtext = reply['snippet']['textDisplay']\n",
    "                rtext = clean(rtext, no_emoji=True)\n",
    "                rlike = reply['snippet']['likeCount']\n",
    "                # print(rtext, rlike)\n",
    "                df = df.append({\"title\": title, \"Comments\": rtext,\n",
    "                               \"likes\": rlike}, ignore_index=True)\n",
    "\n",
    "        df = df.append({\"title\": title, \"Comments\": comment_text,\n",
    "                       \"likes\": likes_count}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "search_terms = [product] + [\"review\"]\n",
    "vid_id = youtube_search(search_terms, 5)\n",
    "df_comments = youtube_comments(vid_id)\n",
    "save_data(df_comments, \"youtube comments\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scrapping all this websites, the following csvs were saved into <b> Data </b>:\n",
    "\n",
    "<b> amazon_reviews.csv </b> : Each row contains the Title of the item , the Link to it , and the Stars and the review for each comment.\n",
    "\n",
    "<b> reddit_scrapped.csv </b> : Each row contains the Posts that the comment comes from, as well as the comment itself.\n",
    "\n",
    "<b> youtube comments.csv </b> : Each row contains the title the comment comes from , the comment itself and the likes each comment received.\n",
    "\n",
    "<b> hardware_zone.csv </b> : Contains the Comment from each post in the forum\n",
    "\n",
    "<b> ~~apple_insider_scraped.csv </b> : Initial scrape of apple insider. Will not be used subsequently.~~\n",
    "\n",
    "All the data are currently very messy and have different headers for storing the same kind of information. The csvs will be cleaned and stored in <b> First_pass </b> after processing and standardisation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are processing all the information collected from the previous step so we have to take the following inputs from the previous steps:\n",
    "\n",
    "<b> product </b>: A string of the product used for analysis, this was generated before step 3. \n",
    "\n",
    "<b> competitors </b>: A dictionary of competing products and their associated company. This was generated by openai in step 3.\n",
    "\n",
    "<b> CSVs of comments </b>: various csvs from containing the comments scrapped from various websites. This will be found in the root directory of the <b> Data  </b> folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"Dove hair\"\n",
    "\n",
    "competitors = {\n",
    "    \"Pantene\": \"Pantene hair\",\n",
    "    \"L' Oreal Paris\": \"L'Oreal Paris hair\",\n",
    "    \"Head & Shoulders\": \"Head & Shoulders hair\",\n",
    "    \"Clear\": \"Clear hair\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 First filter to remove unrelated comments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting all the data, there is a high chance that many comments may not be particularly helpful or even related to the product. Therefore we need to clean the data before we can categorise the comments properly.\n",
    "\n",
    "In order to clasify the comments, a zero-shot classification was used using a pretrained model. It takes certain labels and tries to identify the correlation of each comment to each tag."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Generating labels for zero-shot classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the labels for the sake of data cleaning, there needs to be categories to sort the unrelated comments into. first_filter_keywords is a set of predetermined generic categories meant to catch unrelated comments. From there we add a tag for the <b> product </b> as well as each of its competing products, using the<b> competitors </b> dictionary made using openAI in the earlier steps. \n",
    "\n",
    "The function <b>get_first_filter(product, competitors, filters) </b> takes the product, competitors and the filters and combines them to return a list of the labels that can be used for the first round of zero shot classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the full list of fake categories and actual categories for the product\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "    # file = open(file_path, encoding=\"utf8\")\n",
    "    # cleaned = list(csv.reader(file))[1:]\n",
    "    # return cleaned\n",
    "    \n",
    "\n",
    "first_filter_keywords = [\n",
    "        'first',\n",
    "        'game',\n",
    "        'app',\n",
    "        'Thanks',\n",
    "        'great video',\n",
    "        'links href a',\n",
    "        'subscribed',\n",
    "        'offtopic',\n",
    "    ]\n",
    "\n",
    "def get_first_filter(product, competitors, filters):\n",
    "    filter = []\n",
    "    filter.append(\"Comment about \" + product)\n",
    "    for i in competitors:\n",
    "        filter.append(\"Comment about \" + i)\n",
    "    filter = filter + filters\n",
    "    return filter\n",
    "\n",
    "\n",
    "\n",
    "first_filter = get_first_filter(product, competitors, first_filter_keywords)\n",
    "first_filter\n",
    "\n",
    "#So ummm afer cleaning 4400 comments, 1412 are more than 0.3 and only 464 are related lmao\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The is the first_filter generated for the case of the iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_filter = ['Comment about iphone',\n",
    " 'Comment about Pantene Hair',\n",
    " \"Comment about L' Oreal Paris hair\",\n",
    " 'Comment about Clear hair',\n",
    " 'Comment about Head & Shoulders hair',\n",
    " 'first',\n",
    " 'Thanks',\n",
    " 'links href a',\n",
    " 'video review',\n",
    " 'subscribed',\n",
    " 'offtopic']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Zero shot classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our labels we can run the zero shot classifier on them. \n",
    "\n",
    "<b> The following code was run on collab </b>\n",
    "\n",
    "This allows the progress to be preiodically saved and run in the background as other work is being done on the computer. \n",
    "\n",
    "There is a <b>CSVsForAid </b> folder which is where one would put the csv for processing and there is another <b> Output </b> folder inside the folder where the output of the code is deposited. \n",
    "\n",
    "The <b> save_to_drive(df,filename):</b> function is used to save the existing progress on the dataframe to the drive, allowing for progress to be logged frequently in case collab crashes. \n",
    "\n",
    "The <b> read_from_drive(file_path, specific_column): </b> function is used to load uninitialised csvs which have not been worked on before.\n",
    "\n",
    "The <b> check_progress_csv(file_path):</b> function is used to load csvs with some progress in them which are already stored in the output folder on the drive. \n",
    "\n",
    "The <b> df_labeller_by_20s(filename, factors, specific_column): </b> function is used to label all the data in the csv. It takes in the filename, the first_filter as the factors, requires the specific_column where the comments are. \n",
    "\n",
    "It will output a csv with the columns <b> Comments: </b>  the sentence, <b> label:  </b> which category the sentence falls in and <b> score: </b> how confident the model is in its prediciction for that sentence. \n",
    "\n",
    "The function first checks if existing progress has been made by checking the <b> Output </b> folder with <b> check_progress_csv(file_path):</b>, if no progress has been made, it will use <b> read_from_drive(file_path, specific_column): </b> to get the raw csv and initialise the dataframe properly with the relevant columns as elaborated in the previous paragraph. Anything that has not been labelled will have label and score set to 'Not Done' and an invalid sentence will have its label and score set to 'Not valid Sentence'.\n",
    "\n",
    "It then runs the classifier on each line and saves the csv into the drive once it has made 20 predictions with more than 30% certainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "\n",
    "\n",
    "def save_to_drive(df,filename):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/Output/' + filename + '_output.csv'\n",
    "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "    df.to_csv(f)\n",
    "\n",
    "def read_from_drive(file_path, specific_column):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/' + file_path + '.csv'\n",
    "  df = pd.read_csv(path)\n",
    "  answer = df[[specific_column]]\n",
    "  return answer\n",
    "\n",
    "def check_progress_csv(file_path):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/Output/' + file_path + '_output.csv'\n",
    "  try:\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "  except:\n",
    "    return 1\n",
    "  \n",
    "def df_labeller_by_20s(filename, factors, specific_column):\n",
    "  #Label every 20 sets and then save to the csv\n",
    "  #If df does not exist in proper form first then create\n",
    "  #Find the column to start\n",
    "  \n",
    "  #Open the csv from drive\n",
    "  trail = check_progress_csv(filename)\n",
    "  if isinstance(trail, pd.DataFrame):\n",
    "    print(\"Progress csv found, starting from existing index\")\n",
    "    df_input = check_progress_csv(filename)[[specific_column, 'label', 'score']]\n",
    "  else:\n",
    "    print(\"No progress csv found, reading from folder\")\n",
    "    df_input = read_from_drive(filename, specific_column)\n",
    "    #Initialise the dataframe and add the columns if not available\n",
    "    columns = df_input.columns\n",
    "    if 'label' not in columns:\n",
    "      df_input['label'] = 'Not Done'\n",
    "      df_input['score'] = 'Not Done'\n",
    "\n",
    "  #iterate through the whole df and every 100, save the information\n",
    "  #finding the first instance of Not Done\n",
    "  index = (df_input[df_input.label == \"Not Done\"].index[0])\n",
    "  ending_index = df_input.shape[0]\n",
    "  count = 0\n",
    "\n",
    "  #Proceed to iterate through\n",
    "  while index < ending_index:\n",
    "    if count == 20:\n",
    "      save_to_drive(df_input, filename)\n",
    "      count = 0\n",
    "      now = datetime.now()\n",
    "      dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "      print(\"saved \" + str(index) + \" sentence at \" + dt_string)\t\n",
    "      \n",
    "    try:\n",
    "        sentence = df_input.loc[index][-3]\n",
    "        result = classifier(sentence, factors)\n",
    "        label = result['labels'][0]\n",
    "        score = result['scores'][0]\n",
    "        df_input.loc[index]['label'] = label\n",
    "        df_input.loc[index]['score'] = score\n",
    "        if score >= 0.3:\n",
    "            count += 1\n",
    "            index += 1\n",
    "    except:\n",
    "      df_input.loc[index]['label'] = 'Not Valid Sentence'\n",
    "      df_input.loc[index]['score'] = 'Not Valid Sentence'\n",
    "      print('skipping' + str(index))\n",
    "      index += 1\n",
    "  #Leave here and save one last time to confirm\n",
    "  #function to save\n",
    "  save_to_drive(df_input, filename)\n",
    "  return df_input\n",
    "\n",
    "\n",
    "answer = df_labeller_by_20s(\"youtube_comments\", first_filter, 'comments')\n",
    "answer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the processing was done on the selected the csvs they were downloaded and place back into this folder under <b> First_pass </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Recombining all the data together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was then scanned through and the sentences that make it through to the second round of classification are selected. Only Comments related to the product and whose confidence in classification is more than 30% will be selected for evaluation.\n",
    "\n",
    "The function <b> combine_csvs(folder_location, product) </b> iterates through all csv in the folder_location and uses the product name to reconstruct the label used in the first_filter. It will then return a dataframe with the comments that belong to that label and are more than 30% certain of being in that category.\n",
    "\n",
    "The function <b> save_data(data, file_name) </b> will then take the dataframe returned from  <b> combine_csvs(folder_location, product) </b> and will then store it in the <b> Data/Second_pass </b> folder with the file_name, <b>check.csv </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In between the filtering: combine those related to iphone together and those to the competitors together, dropping the others (only added if more than 0.3 certain between layers)\n",
    "#Standardised all other scrappers to have Comments, label, score tabs\n",
    "import pandas as pd\n",
    "import os\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def combine_csvs(folder_location, product):\n",
    "    product_stuff = pd.DataFrame(columns=['Comments'])\n",
    "    product_filter = \"Comment about \" + product\n",
    "    certainty = 0.3\n",
    "    for filename in os.listdir(folder_location):\n",
    "        print(filename)\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(folder_location, filename)\n",
    "            dataframe = read_csv(filepath) \n",
    "            for index, row in dataframe.iterrows():\n",
    "                comment = str(row['Comments']).strip()\n",
    "                label = row['label']\n",
    "                score = row['score']\n",
    "                if score ==  \"Not Valid Sentence\":\n",
    "                    continue\n",
    "                if score == \"Not Done\":\n",
    "                    break\n",
    "                if float(score) > certainty:\n",
    "                    if label == product_filter:\n",
    "                        product_stuff.loc[len(product_stuff)]  = {\"Comments\": comment}  \n",
    "    return product_stuff\n",
    "    \n",
    "                       \n",
    "this =combine_csvs('Data/First_pass', 'Dove Hair', competitors)     \n",
    "save_data(this, \"Second_pass/check\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from this function is saved in check.csv under Second_pass in Data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Classification back into categories identified in part 2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, it can be put into the categories based on the <b> keywords </b> identified earlier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Labels used for round 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the <b> keywords </b> found in step 3 to serve as the categories to sort the data. As they could still be unrelated comments, a second set of generic filters was created to filter out unwanted comments\n",
    "\n",
    "The function <b>> get_categorization_filter(keywords, filters) </b> takes in the keywords found in step 3 as well as a predefined list of strings which is used as the second round of filtering. It then returns the list of categories that will be used for the second round of zero-shot classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorization_filter(keywords, filters):\n",
    "    final_filter = keywords + filters\n",
    "    return final_filter\n",
    "\n",
    "#might need a second filter ? using first_filter_keywords as a stand in\n",
    "#keywords come from create_keywords in 2c\n",
    "\n",
    "second_filter_keywords = [\n",
    "    'generic',\n",
    "    'popularity',\n",
    "    'links',\n",
    "    'offtopic',\n",
    "    'suggestion',\n",
    "    'advice'\n",
    "]\n",
    "\n",
    "\n",
    "categories = get_categorization_filter(keywords, second_filter_keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the output for the iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['heavy',\n",
    " 'price',\n",
    " 'camera',\n",
    " 'durable',\n",
    " 'durability',\n",
    " 'lightweight',\n",
    " 'easy to use',\n",
    " 'craftsmanship',\n",
    " 'easy to set up',\n",
    " 'charging speed',\n",
    " 'charges quickly',\n",
    " 'water resistance',\n",
    " 'wireless charging',\n",
    " 'long battery life',\n",
    " 'comfortable to use',\n",
    " 'temperature control',\n",
    " 'generic',\n",
    " 'popularity',\n",
    " 'links',\n",
    " 'offtopic',\n",
    " 'suggestion',\n",
    " 'advice']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Zero shot classification round 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code still remains the same as the first zero shot classification, however the factors used has changed from <b> first_filter </b> to <b> categories </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "\n",
    "\n",
    "def save_to_drive(df,filename):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/Output/' + filename + '_output.csv'\n",
    "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "    df.to_csv(f)\n",
    "\n",
    "def read_from_drive(file_path, specific_column):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/' + file_path + '.csv'\n",
    "  df = pd.read_csv(path)\n",
    "  answer = df[[specific_column]]\n",
    "  return answer\n",
    "\n",
    "def check_progress_csv(file_path):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/Output/' + file_path + '_output.csv'\n",
    "  try:\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "  except:\n",
    "    return 1\n",
    "  \n",
    "def df_labeller_by_20s(filename, factors, specific_column):\n",
    "  #Label every 20 sets and then save to the csv\n",
    "  #If df does not exist in proper form first then create\n",
    "  #Find the column to start\n",
    "  \n",
    "  #Open the csv from drive\n",
    "  trail = check_progress_csv(filename)\n",
    "  if isinstance(trail, pd.DataFrame):\n",
    "    print(\"Progress csv found, starting from existing index\")\n",
    "    df_input = check_progress_csv(filename)[[specific_column, 'label', 'score']]\n",
    "  else:\n",
    "    print(\"No progress csv found, reading from folder\")\n",
    "    df_input = read_from_drive(filename, specific_column)\n",
    "    #Initialise the dataframe and add the columns if not available\n",
    "    columns = df_input.columns\n",
    "    if 'label' not in columns:\n",
    "      df_input['label'] = 'Not Done'\n",
    "      df_input['score'] = 'Not Done'\n",
    "\n",
    "  #iterate through the whole df and every 100, save the information\n",
    "  #finding the first instance of Not Done\n",
    "  index = (df_input[df_input.label == \"Not Done\"].index[0])\n",
    "  ending_index = df_input.shape[0]\n",
    "  count = 0\n",
    "\n",
    "  #Proceed to iterate through\n",
    "  while index < ending_index:\n",
    "    if count == 20:\n",
    "      save_to_drive(df_input, filename)\n",
    "      count = 0\n",
    "      now = datetime.now()\n",
    "      dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "      print(\"saved \" + str(index) + \" sentence at \" + dt_string)\t\n",
    "      \n",
    "    try:\n",
    "        sentence = df_input.loc[index][-3]\n",
    "        result = classifier(sentence, factors)\n",
    "        label = result['labels'][0]\n",
    "        score = result['scores'][0]\n",
    "        df_input.loc[index]['label'] = label\n",
    "        df_input.loc[index]['score'] = score\n",
    "        if score >= 0.3:\n",
    "            count += 1\n",
    "            index += 1\n",
    "    except:\n",
    "      df_input.loc[index]['label'] = 'Not Valid Sentence'\n",
    "      df_input.loc[index]['score'] = 'Not Valid Sentence'\n",
    "      print('skipping' + str(index))\n",
    "      index += 1\n",
    "  #Leave here and save one last time to confirm\n",
    "  #function to save\n",
    "  save_to_drive(df_input, filename)\n",
    "  return df_input\n",
    "\n",
    "\n",
    "answer = df_labeller_by_20s(\"youtube_comments\", categories, 'comments')\n",
    "answer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for this function is then downloaded and stored inside <b> Data/Final </b> as <b> check_output.csv </b>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing, all the data, the comments that will be used to derive our design opportunities for the product have finally been separated out. \n",
    "\n",
    "<b> check_output.csv </b>: This is the csv holding all the data, it has the comment, the label for what categories it belongs to and the confidence score. It is located in the <b> Data/Final</b> folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identification of areas of opportunity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, take all the the information collected from the previous steps and desrive the outputs.\n",
    "\n",
    "<b> product </b>: A string of the product used for analysis, this was generated before step 3. \n",
    "\n",
    "<b> keywords </b>: a list of keywords generated as the ouput of step 3, this will be used as the categories to filter the comments into\n",
    "\n",
    "<b> competitors </b>: A dictionary of competing products and their associated company. This was generated by openai in step 3.\n",
    "\n",
    "<b> CSVs of final comments </b>: This is the csv of the comments that we will be analysing. It can be found in the <b> Data/Final  </b> folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'Dove hair'\n",
    "\n",
    "keywords = [\"Hair type\", \"Ingredients\", \"Brand Reputation\", \"Gentle formula\", \"Moisturizing properties\", \"Product claims\"]\n",
    "\n",
    "competitors = {\n",
    "    \"Pantene\": \"Pantene hair\",\n",
    "    \"L' Oreal Paris\": \"L'Oreal Paris hair\",\n",
    "    \"Head & Shoulders\": \"Head & Shoulders hair\",\n",
    "    \"Clear\": \"Clear hair\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0.1 Intended Outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several outputs that will be delivered\n",
    "\n",
    "<b> 1. The top three categories that future iterations of the product should focus on:</b> This can be determined by performing a quantitative analysis of the data.  The most important categories will be the categories with the highest absolute score as they will be overwhelmingly positive or negative. We will then return the top 3 categories as the categories they should focus on.\n",
    "\n",
    "<b> 2. The ways to improve the top three categories that future iterations of the product:</b> \n",
    "This can be done using a qualitative analysis. We will take all the comments said about the top categories and group them together before summarising them and finding out what is the best way to improve the category.\n",
    "\n",
    "<b> 3.  A design problem statement to start designers ideating on improve the product:</b> \n",
    "Using the categories in output 1, we can use openai to generate a design prompt for designers to ideate on.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Sentiment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use a sentiment analysis model to analyse the data and label each comment positive or negative.\n",
    "\n",
    "We can then group the comments by the category they belong to and assign a score to each category\n",
    "\n",
    "The function <b> sentiment_labeller(df, categories): </b> takes the dataframe of the csv created in the previous steps and the keywords as the categories. It then outputs a dictionary of Scores for each category, a dictionary of all the positive things said for each category and a dictionary of all the negative things said in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "#Sentiment analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "processed = read_csv('Data\\Final\\check_output.csv')\n",
    "\n",
    "\n",
    "#The goal of this function is to take a dataset and for each comment in it label it is something positive or negative, the data would then be send to the word storages \n",
    "#df is the dataframe to be labelled and categories is the processed keywords\n",
    "#categories\n",
    "def sentiment_labeller(df, categories):\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    sentences = []\n",
    "    for index, row in df.iterrows():\n",
    "        sentence, category = row['Comments'], row['label']\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    #Takes the labels positive and negative and adds it as a new column to the dataframe supplied\n",
    "    new_list = list(map(lambda x: x['label'], sentiment_pipeline(sentences)))\n",
    "    df['Sentiment'] = new_list\n",
    "    \n",
    "    #Now create the stuff to store the things in\n",
    "    #postive_word_storage is the compliments in the category\n",
    "    #negative_word_storage is the complains in the category\n",
    "    #storage is just the score\n",
    "    storage = {}\n",
    "    positive_word_storage = {}\n",
    "    negative_word_storage = {}\n",
    "    for x in categories:\n",
    "        storage[x] = 0\n",
    "        positive_word_storage[x] = \"\"\n",
    "        negative_word_storage[x] = \"\"\n",
    "        \n",
    "    #iterate through all the rows\n",
    "    for index, row in df.iterrows():\n",
    "        comment = row['Comments']\n",
    "        category = row['label']\n",
    "        positiveNegative = row['Sentiment']\n",
    "        active = None\n",
    "        if category in categories:\n",
    "            if positiveNegative == \"POSITIVE\":\n",
    "                storage[category] += 1\n",
    "                active = positive_word_storage\n",
    "            else:\n",
    "                storage[category] -= 1\n",
    "                active = negative_word_storage\n",
    "            comment = comment.strip()\n",
    "            if comment[-1] in string.punctuation:\n",
    "                active[category] = active[category] + comment\n",
    "            else:\n",
    "                active[category] = active[category] + comment + '.'\n",
    "        Qualatative = sorted(list(storage.items()), key= lambda x: abs(x[1]), reverse=True)\n",
    "    return Qualatative, positive_word_storage, negative_word_storage\n",
    "#Quantitative data analysed, now go find the largest numerical category and if they are good or bad\n",
    "\n",
    "Qualatative, positive, negative = sentiment_labeller(processed,keywords)\n",
    "\n",
    "#Done once for the iphone and once for the competition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Getting suggestions for improvement based on comments and design prompt (qualitative analysis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b> get_best_way_to_improve_quality(points, extracted_comments, product): </b> takes the output of the previous function as well as the products. It then creates a prompt to ask openai to summarise the paragraph and give the best way for a company to improve the product based on the comments.\n",
    "\n",
    "The <b>def get_design_problem_statement(product, points): </b> takes the products as well as the top 3 categories and uses them to prompt openai to generate a design problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function then takes the points and extracted comments and ask chatgpt to summarise the best way to improve in the various categories\n",
    "def get_best_way_to_improve_quality(points, extracted_comments, product):\n",
    "    ways_to_improve = {}\n",
    "    for i in range(len(points)):\n",
    "        prompt = \"According to the extracted comments, what is the best way for the company of the \" + product + \" to improve following aspect of the \"+ product  + \"? Aspect: \" + points[i] + \". Extracted Comments: \" + extracted_comments[i]\n",
    "        response = ask_gpt(prompt)\n",
    "        print(response)\n",
    "        ways_to_improve[points[i]] = response\n",
    "    return ways_to_improve\n",
    "\n",
    "responsed = get_best_way_to_improve_quality(x,y, product)\n",
    "print(responsed)\n",
    "\n",
    "#This function will take the points and use openai to create a design prompt for the product using the points extracted previously\n",
    "def get_design_problem_statement(product, points):\n",
    "    pointers = \"\"\n",
    "    for i in points:\n",
    "        pointers = pointers + i\n",
    "    prompt = \"Create a Design problem statement to improve the \" + product + \" centering around the following qualities: \" + pointers\n",
    "    return ask_gpt(prompt)\n",
    "    \n",
    "response = get_design_problem_statement(product, x)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Outputs and reflections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to take the reponses and consolidate them into a single output in the form of text file\n",
    "\n",
    "The following code takes the responses collected and creates the <b> output.txt </b> file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    f.write('This is the output for the product: ')  \n",
    "    f.write(product+ '\\n\\n\\n') \n",
    "    \n",
    "    f.write(\"Output one: Product's weakest areas\" + '\\n')\n",
    "    f.write('According to the comments analysed, the product is weakest in the following categories: ' + '\\n\\n')\n",
    "    \n",
    "    \n",
    "    categories_to_improve = \"\"\n",
    "    for i in range(len(x)):\n",
    "        categories_to_improve = categories_to_improve + x[i]\n",
    "        if i != len(x) - 1:\n",
    "            categories_to_improve = categories_to_improve + ', '\n",
    "            \n",
    "    f.write(categories_to_improve + '\\n\\n')\n",
    "    \n",
    "    f.write('\\n\\n')\n",
    "\n",
    "    \n",
    "    f.write(\"Output two: Suggested means to improve the product's weakest areas:\" + '\\n')\n",
    "    f.write('These are the suggestions for a company to improve in the following categories: ' + '\\n\\n')\n",
    "    for j in responsed:\n",
    "        f.write(j + \" : \" + responsed[j] + '\\n\\n' )\n",
    "        \n",
    "    f.write('\\n\\n')\n",
    "\n",
    "    f.write(\"Output three: A design problem statement to start designers ideating on improve the product:\" + '\\n')\n",
    "    f.write(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

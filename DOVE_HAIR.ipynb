{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dove Hair"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The code web scrapes 1000s of comments and so, the total output of it hasn't been generated. The github link will be updated with the latest outputs as and when they are done."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Installing libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below are several libraries that were used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install amazon_search_results_scraper\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install selenium\n",
    "# !pip install clean-text\n",
    "# !pip install langdetect\n",
    "# !pip install google-api-python-client\n",
    "# !pip install youtube-transcript-api\n",
    "# !pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def create_directories():\n",
    "    try:\n",
    "        os.makedirs(\"Data\")\n",
    "        print(\"Base directory created\")\n",
    "    except:\n",
    "        print(\"base directory: Data exists\")\n",
    "    folders = ['Final', 'First_pass', 'Second_pass']\n",
    "    for i in folders:\n",
    "        path = 'Data'\n",
    "        try:\n",
    "            os.makedirs(os.path.join(path, i))\n",
    "        except:\n",
    "            print(\"Folder \" + i + \" exists.\")\n",
    "\n",
    "create_directories()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Pre-analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific product is the only input needed for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'Dove Shampoo'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Forming the categories to classify comments into"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to commencing the analysis, it is crucial to obtain comprehensive knowledge of the existing product that requires improvement. This will facilitate the creation of specific categories to analyze the product effectively by classifying the scraped comments accurately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Using openAI to get the strengths, flaws and competitors of the product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial step of this study involved utilizing the OpenAI library to identify the strengths and weaknesses of the product under examination. Specific prompts were used to ensure that the response generated by OpenAI was in a format that could be easily converted into a Python dictionary. Additionally, information on the competitors was also gathered for use in subsequent data-cleaning steps.\n",
    "\n",
    "A general function named \"ask_gpt(prompt)\" was developed to send prompts to the OpenAI website and retrieve the generated response in the form of a string. This function was subsequently used in three other functions: \"get_design_strength(keyword),\" \"get_design_flaws(keyword),\" and \"get_competitors (keyword).\" Each of these functions accepts a product keyword (e.g., 'Dove Shampoo') as input and returns a dictionary string created from the response to the prompt generated within the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\", prompt=prompt, max_tokens=1024, n=1, stop=None, temperature=0.1\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "\n",
    "def get_design_strength(keyword):\n",
    "    return ask_gpt(\"Give me the design strengths of the\" + keyword + \"in a python dictionary format\")\n",
    "\n",
    "def get_design_flaws(keyword):\n",
    "    return ask_gpt(\"Give me the design flaws of the\" + keyword + \"in a python dictionary format\")\n",
    "\n",
    "def get_competitors(keyword):\n",
    "    return ask_gpt(\"Give me the competitors of the\" + keyword + \"in a python dictionary format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = get_design_flaws(\"Dove Hair Therapy Breakage Remedy Shampoo with Bio-Cellular Complex & Vitamin C\")\n",
    "response3 = get_design_strength(\"Dove Hair Therapy Breakage Remedy Shampoo with Bio-Cellular Complex & Vitamin C\")\n",
    "response4 = get_competitors(\"Dove Hair Therapy Breakage Remedy Shampoo with Bio-Cellular Complex & Vitamin C\")\n",
    "print(response2)\n",
    "print(response3)\n",
    "print(response4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from the openAI is currently a string. So the ast library is used to convert it from a string into a python dictionary.\n",
    "\n",
    "The function <b> convert_string_dict(string) </b> will then take the string returned from openai and convert it into a dictionary. Do note that the string has to be in a specific format to return a python dictionary properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert_string_to_dict(string):\n",
    "    try:\n",
    "        index = string.index(\"=\")\n",
    "        string = string[index+1:].strip()\n",
    "    except:\n",
    "        #no =\n",
    "        pass\n",
    "    return ast.literal_eval(string)\n",
    "\n",
    "design_flaws =(convert_string_to_dict(response2))\n",
    "design_strengths = (convert_string_to_dict(response3))\n",
    "competitors = (convert_string_to_dict(response4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Scrape google shopping to get categories "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code performs web scraping of Google Shopping reviews for products that match certain search terms using Selenium and BeautifulSoup. The code is divided into several functions.\n",
    "\n",
    "The “save_data()” function takes a DataFrame and a file name as arguments, creates a directory named after the search terms, and saves the data to a CSV file in the directory.\n",
    "\n",
    "The “insearch_result()” function takes a list of search terms and a title and returns a boolean indicating whether all the search terms are in the title.\n",
    "\n",
    "The “googleshopping_get_id()” function takes a list of search terms, opens a Chrome browser using Selenium, navigates to the Google Shopping page, and enters the search terms into the search bar. It then waits for the page to load, scrapes the HTML using BeautifulSoup, and extracts the product IDs for each product on the page that matches the search terms. It returns a list of product IDs.\n",
    "\n",
    "The “googleshopping_get_keywords()” function takes a list of product IDs and a list of search terms and scrapes the Google Shopping reviews page for each product that matches the search terms. For each review, it extracts the title, number of reviews, keywords, percentage, and positive/negative sentiment. It creates a data frame with these values and returns it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_data(data, file_name):\n",
    "    try:                                            # Create directory named after search terms\n",
    "        os.makedirs(\"Data\")\n",
    "        print(\"Directory created\")\n",
    "\n",
    "    except FileExistsError:\n",
    "        print(\"Directory exists\")\n",
    "\n",
    "    data.to_csv(\"Data/%s.csv\" % (file_name))\n",
    "\n",
    "def insearch_result(search_term, title):\n",
    "    title = title.lower()\n",
    "    for keywords in search_term:\n",
    "        if keywords not in title:\n",
    "            # print(title)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def googleshopping_get_id(search_terms):\n",
    "    base_url = \"https://shopping.google.com/\"\n",
    "\n",
    "    # initialising chrome and going to the url\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # finding search bar and searching for search_terms\n",
    "    search = driver.find_element(\"xpath\", \"//*[@id='REsRA']\")\n",
    "    # print(\"The input Element is: \", search)\n",
    "    search.send_keys(\" \".join(search_terms))\n",
    "    search.send_keys(Keys.RETURN)\n",
    "\n",
    "    # pausing to let the page load and scrape the page html\n",
    "    time.sleep(5)\n",
    "    page = driver.page_source\n",
    "\n",
    "    # close the driver\n",
    "    time.sleep(2)\n",
    "    driver.close()\n",
    "\n",
    "    # parse the page\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    product_id = []\n",
    "    for links in soup.find_all('a', href=True):\n",
    "        if \"/shopping/product\" in links['href']:\n",
    "            end_index = links['href'].index(\"?\")\n",
    "            id = links['href'][18:end_index]\n",
    "\n",
    "            if id not in product_id:\n",
    "                product_id.append(id)\n",
    "\n",
    "    return product_id\n",
    "\n",
    "\n",
    "def googleshopping_get_keywords(productID, search_terms):\n",
    "    # Require a list of ProductID\n",
    "    base_url = \"https://www.google.com/shopping/product/\"\n",
    "    df = pd.DataFrame(columns=[\"Title\", \"Number of Reviews\",\n",
    "                      \"Keywords\", \"Percentage\", \"Positive/Negative\"])\n",
    "\n",
    "    for i in range(len(productID)):\n",
    "\n",
    "        if \"/offers\" in productID[i]:\n",
    "            index = productID[i].index(\"/offers\")\n",
    "            productID[i] = productID[i][:index]\n",
    "\n",
    "        url = base_url + productID[i] + \"/reviews\"\n",
    "        # print(url)\n",
    "\n",
    "        driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        page = driver.page_source\n",
    "        time.sleep(2)\n",
    "        driver.close()\n",
    "\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        title = soup.find(\"div\", {\"class\": \"f0t7kf\"}).text\n",
    "        anyreviews = soup.find(\"div\", {\"class\": \"rktlcd\"})\n",
    "\n",
    "        if anyreviews == None:\n",
    "            # print(\"No reviews found\")\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            intitle = insearch_result(search_terms, title)\n",
    "\n",
    "            if intitle:\n",
    "                for span in soup.find_all(\"span\", \"QIrs8\"):\n",
    "                    text = span.text\n",
    "                    # print(text)\n",
    "                    if text == \"Select to view all reviews\":\n",
    "                        pass\n",
    "\n",
    "                    else:\n",
    "                        # print(text)\n",
    "                        reviews_index = text.index(\"r\")\n",
    "                        about_index = text.index(\"about\")\n",
    "                        full_stop_index = text.index(\".\")\n",
    "                        percentage_index = text.index(\"%\")\n",
    "\n",
    "                        num_of_reviews = text[4: reviews_index].strip()\n",
    "                        keywords = text[about_index +\n",
    "                                        len(\"about\")+1: full_stop_index]\n",
    "                        percentage = text[full_stop_index+2: percentage_index]\n",
    "                        positivenegative = text[-9:-1]\n",
    "                        # print(num_of_reviews, keywords,\n",
    "                        #       percentage, positivenegative)\n",
    "                        df = df.append({\"Title\": title, \"Number of Reviews\": num_of_reviews, \"Keywords\": keywords,\n",
    "                                       \"Percentage\": percentage, \"Positive/Negative\": positivenegative}, ignore_index=True)\n",
    "                        # print(df)\n",
    "    return df\n",
    "\n",
    "search_terms = ['dove shampoo']\n",
    "product_ids = googleshopping_get_id(search_terms)\n",
    "data = googleshopping_get_keywords(product_ids, search_terms)\n",
    "save_data(data, \"google_shopping_scraped\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the search_terms list is set to ['Dove Shampoo'], and the “googleshopping_get_id()” and “googleshopping_get_keywords()” functions are called with this list. The resulting DataFrame is then saved to a CSV file using the “save_data()” function with the file name \"google_shopping_scraped\".\n",
    "\n",
    "Next, “extract_keywords_from_google_shopping_csv takes the above file’s location as input and returns a list of unique keywords present in the fourth column of the CSV file.\n",
    "The pandas library is used to read the CSV file into a data frame, which is then iterated over using a for-loop. The value in the fourth column of each row is stored as a keyword. If the keyword is not already in the keywords list, it is added to the list. Finally, the keywords list is returned.\n",
    "The last line of the script calls the “extract_keywords_from_google_shopping_csv” function with the file location \"Data\\google_shopping_scraped.csv\" and stores the result in a variable called “google_keywords”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def extract_keywords_from_google_shopping_csv(file_location):\n",
    "    keywords = []\n",
    "    goolge_pd = pd.read_csv(file_location)\n",
    "    for index, row in goolge_pd.iterrows():\n",
    "        keyword = row[3]\n",
    "        if keyword not in keywords:\n",
    "            keywords.append(keyword)\n",
    "    return keywords\n",
    "\n",
    "google_keywords = (extract_keywords_from_google_shopping_csv(\"Data\\google_shopping_scraped.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Combine all keywords found (removing repeats) and use siamese network to remove similar categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function “combine_keywords (strengths: list, flaws: list, google_keywords: list)” then takes lists of the “strengths”, “flaws”, and “google_keywords” generated from the previous functions and returns a list of the compiled keywords.\n",
    "\n",
    "The function “combine_keywords” takes three arguments: “strengths”, “flaws”, and “google_keywords”. The function first initializes an empty list called “keywords”. It then loops through each item in the “strengths” list and adds it to the “keywords” list if it is not already there (converted to lowercase). It does the same for the flaws list and the “google_keywords” list. Finally, the function returns the “keywords” list, which contains all of the unique keywords from the three input lists.\n",
    "\n",
    "However, in the output, it was observed that certain keywords were quite similar. A Siamese Network was then used to identify keywords that were similar and remove them.\n",
    "\n",
    "The function “siamese_analysis (All_keywords)” uses the GloVe model from the gensim library to create vector representations of these unique keywords. It then calculates the cosine similarity between each pair of vectors and removes keywords that have a similarity score below a threshold of 0.2. The resulting list of  “cleaned_keywords” contains only the keywords that are dissimilar enough from each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_keywords(strengths: list,flaws:list, google_keywords: list):\n",
    "    keywords = []\n",
    "    for strength in strengths:\n",
    "        if strength not in keywords:\n",
    "            keywords.append(strength).lower()\n",
    "    for flaw in flaws:\n",
    "        if flaw not in keywords:\n",
    "            keywords.append(flaw).lower()\n",
    "    for goolge in google_keywords:\n",
    "        if goolge not in keywords:\n",
    "            keywords.append(goolge).lower()\n",
    "    return keywords\n",
    "\n",
    "All_keywords = (combine_keywords(design_flaws.keys(),design_strengths.keys(),google_keywords))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the ouptut from combine keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_keywords = [\"Hair type\", \"Ingredients\", \"Brand Reputation\", \"Gentle formula\", \"Moisturizing properties\", \"Product claims\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what was observed is that certain keywords were quite similar. A siamese network was then used to identify keywords that were similar and remove them. \n",
    "\n",
    "<b> This code was run on collab. </b>\n",
    "\n",
    "The function <b> siamese_analysis(All_keywords) </b> takes the list of all the keywords from the previous steps and returns a list of keywords that has been cleaned with a siamese analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go to Collab\n",
    "from scipy import spatial\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "def siamese_analysis(All_keywords):\n",
    "  model = api.load(\"glove-wiki-gigaword-50\") #choose from multiple models https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "  keywords = All_keywords\n",
    "  def preprocess(s):\n",
    "    return [i.lower() for i in s.split()]\n",
    "\n",
    "  def get_vector(s):\n",
    "    return np.sum(np.array([model[i] for i in preprocess(s)]), axis=0)\n",
    "\n",
    "  vector_data = []\n",
    "  for keyword in keywords:\n",
    "    print(keyword)\n",
    "    \n",
    "    value = get_vector(keyword)\n",
    "    vector_data.append(value)\n",
    "    \n",
    "\n",
    "  cleaned_keywords = []\n",
    "\n",
    "  for i in range(len(vector_data)):\n",
    "    for j in range(i, len(vector_data)):\n",
    "      similarity = 1 - spatial.distance.cosine(vector_data[i], vector_data[j])\n",
    "      if similarity < 0.2:\n",
    "        cleaned_keywords.append(keywords[i])\n",
    "        cleaned_keywords.append(keywords[j])\n",
    "        # print(keywords[i], keywords[j])\n",
    "\n",
    "\n",
    "  cleaned_keywords = list(set(cleaned_keywords))\n",
    "  return cleaned_keywords\n",
    "\n",
    "keywords = All_keywords\n",
    "cleaned_keywords = siamese_analysis(keywords)\n",
    "# print(cleaned_keywords)\n",
    "print(len(keywords), len(cleaned_keywords))\n",
    "print(cleaned_keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the cleaned keywords from the run on the platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_keywords = [\"Hair type\", \"Ingredients\", \"Brand Reputation\", \"Gentle formula\", \"Moisturizing properties\", \"Product claims\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Remove categories that are substring of other categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were still a few keyword categories that were repetitive as they were substrings of other keywords. Thus, the function  “create_keywords” was created to take the cleaned keywords from the previous step and remove the shorter string in the case that one string was the substring of another string. This was done to remove redundant keywords and ensure that only the most informative keywords are used in the analysis. It then returned the list of the final keywords which will be stored in “variable_keywords”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keywords(cleaned_keywords):\n",
    "    cleaned_keywords.sort(key=len, reverse=False)\n",
    "\n",
    "    for i in range(len(cleaned_keywords)):\n",
    "        cleaned_keywords[i] = cleaned_keywords[i].lower()\n",
    "    print(cleaned_keywords)\n",
    "\n",
    "    # loop through each string in the list\n",
    "    for i in range(len(cleaned_keywords)):\n",
    "        # compare the string with all subsequent strings in the list\n",
    "        for j in range(i+1, len(cleaned_keywords)):\n",
    "            if cleaned_keywords[i] in cleaned_keywords[j]:\n",
    "                # if the current string is a substring of another string, remove it from the list\n",
    "                cleaned_keywords.pop(i)\n",
    "                break\n",
    "        else:\n",
    "            # if the current string is not a substring of any subsequent strings, move on to the next string\n",
    "            continue\n",
    "        # if the current string was removed from the list, adjust the index accordingly\n",
    "        i -= 1\n",
    "    return cleaned_keywords\n",
    "\n",
    "keywords = create_keywords(cleaned_keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the keywords that were generated in the initial run of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Hair type\", \"Ingredients\", \"Brand Reputation\", \"Gentle formula\", \"Moisturizing properties\", \"Product claims\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the input of the product, we were able to fully automate the process of getting the following outputs:\n",
    "\n",
    "“google_shopping_scraped.csv” is saved in the Data Folder with categories and a number of reviews under each category.\n",
    "\n",
    "product: A string of the name of the product that will be the focus of the study. For the sake of this report, this product will be 'Dove Shampoo'\n",
    "\n",
    "keywords: A list of keywords that can be used for classification using zero-shot or other text analysis models.\n",
    "\n",
    "competitors: A dictionary of competing products and companies that can be used for data screening.\n",
    "These are the following outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Hair type\", \"Ingredients\", \"Brand Reputation\", \"Gentle formula\", \"Moisturizing properties\", \"Product claims\"]\n",
    "\n",
    "competitors = {\n",
    "    \"Pantene\": \"Pantene hair\",\n",
    "    \"L' Oreal Paris\": \"L'Oreal Paris hair\",\n",
    "    \"Head & Shoulders\": \"Head & Shoulders hair\",\n",
    "    \"Clear\": \"Clear hair\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Big Data collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to step 2, we are still collecting data and all we need is the product name (for the general websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'Dove shampoo'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Choice of websites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to gather data to analyze ways to improve the product, a variety of websites were selected for data collection. To ensure that the dataset would be applicable to any product, several generic websites were chosen. \n",
    "\n",
    "These included Amazon, a popular e-commerce website that features a wide range of products. It should be noted that Amazon does have an anti-scraping policy, but this project was conducted for educational purposes and not for commercial gain.\n",
    "\n",
    "In addition to Amazon, Reddit was selected as it features discussions on a wide range of topics and is one of the more popular forum sites. Youtube, a popular video playback site, was also selected as it features a range of product reviews that would be prime for comment scrapping.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Scrapping the websites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Amazon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the “amazon_search_results_scraper” API, we were able to find the product pages and then use Selenium to collect the comments under the product pages found. The code scrapes product reviews for Dove Shampoo from Amazon. It imports the necessary packages such as pandas, BeautifulSoup, time, selenium and Webdriver to achieve its goal. The script defines several functions to check the validity of the links, to create a link to a custom reviews page for each valid link, to create links to multiple pages, and to scrape reviews from each of the links. The main function “search_amazon()” accepts a keyword to be searched, which is the string \"Dove Shampoo\" in this case, and a list of exceptions to exclude from the search. The function  “save_data(data, file_name)”  will then take the data frame returned from “search_amazon(keyword, exceptions)”  and will then store it in the  Data folder with the file_name, “amazon_reviews.csv”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_search_results_scraper import *\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import random\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "def checkvalid_us(main_keyword, i):\n",
    "    try:\n",
    "        title = i['link']\n",
    "        if main_keyword in title:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def create_link_to_crpage_us(link):\n",
    "    new_link = link.replace(\"dp\", \"product-reviews\")\n",
    "    new_link, chop, chop_liver = new_link.partition(\"ref=\")\n",
    "    new_link = new_link + \\\n",
    "        'ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&pageNumber=1&sortBy=recent'\n",
    "    return new_link\n",
    "\n",
    "def amazon_create_different_page_links(link, number_of_pages_iteration=5):\n",
    "    links = []\n",
    "    links.append(link)\n",
    "    for i in range(2, (number_of_pages_iteration + 1)):\n",
    "        linked = links[-1]\n",
    "        new_link = linked.replace(\n",
    "            'pageNumber=' + (str(i-1)), 'pageNumber=' + (str(i)))\n",
    "        links.append(new_link)\n",
    "    return links\n",
    "    \n",
    "def search_amazon(keyword, exceptions):\n",
    "    amazon.open(\"https://www.amazon.com/\")\n",
    "    main_keyword = keyword\n",
    "    title_exceptions = exceptions\n",
    "    amazon.search(keyword=main_keyword)\n",
    "\n",
    "    response = amazon.search_results()\n",
    "    search_results = response['body']\n",
    "\n",
    "    pages = []\n",
    "    for i in search_results:\n",
    "        if checkvalid_us(main_keyword, i):\n",
    "            pages.append(i['link'])\n",
    "        else:\n",
    "            pass\n",
    "            # print('no valid link or no valid_title')\n",
    "\n",
    "    # print(create_link_to_crpage_us(pages[1]))\n",
    "\n",
    "    custom_review_pages = []\n",
    "    for i in pages:\n",
    "        to_add = create_link_to_crpage_us(i)\n",
    "        custom_review_pages.append(to_add)\n",
    "    \n",
    "    #links stored in custom_review_pages\n",
    "    df = pd.DataFrame(columns=[\"Title\" , \"Link\", \"Stars\", \"Comments\"])\n",
    "    list_links = []\n",
    "    for link in custom_review_pages:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\n",
    "        'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(link)\n",
    "        htmlsource = driver.page_source\n",
    "        soup = BeautifulSoup(htmlsource, 'html.parser')\n",
    "        driver.quit()\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            title = soup.find('h1')\n",
    "            title = (title.findChildren('span')[0].text.strip())\n",
    "            review_section = soup.find('div', {'id': \"cm_cr-review_list\"})\n",
    "            # find all reviews\n",
    "            reviews = review_section.find_all('div', {'class': \"review\"})\n",
    "            # for each review\n",
    "            for i in reviews:\n",
    "                # collect stars\n",
    "                stars = i.find('i', {'class': 'review-rating'})\n",
    "                stared = (stars.findChildren('span')[0]).text.strip()\n",
    "                # collect review\n",
    "                review = i.find(\n",
    "                    'a', {'class': 'a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold'})\n",
    "                review = (review.findChildren(\n",
    "                    'span', recursive=True)[0]).text.strip()\n",
    "                df.loc[len(df)]  = {\"Title\": title, \"Link\": link, \"Stars\": str(stared), \"Comments\":str(review)}  \n",
    "        except:\n",
    "            print(soup)\n",
    "        time.sleep(10)\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_reviews =search_amazon('dove shampoo', ['Case', 'case'])\n",
    "save_data(df_amazon_reviews, \"amazon_reviews\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Reddit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Python Reddit API Wrapper (PRAW), the code performs web scraping of comments related to a particular topic on Reddit. It has three functions:\n",
    "\n",
    "“scrape_reddit(search_term: str, post_limit=10)” –– This function takes a search term and a post limit as input and returns a Pandas DataFrame containing the title of the post and all the comments for the top post_limit hot posts on the subreddit with the name search_term.\n",
    "\n",
    "“cleanup_reddit(uncleaned_frame)” –– This function takes an uncleaned DataFrame and returns a cleaned DataFrame that contains the title of the post and the comments for each post in separate rows. The comments are cleaned using the clean text package and then split into multiple rows if they contain multiple sentences.\n",
    "\n",
    "“combined_reddit(search_term: str, post_limit=10)” –– This function combines the above two functions and returns the cleaned DataFrame. Finally, the cleaned DataFrame is saved to a file using the save_data() function.\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import re\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "\n",
    "def scrape_reddit(search_term:str, post_limit=10):\n",
    "    df = pd.DataFrame(columns=['Posts', 'Comments'])\n",
    "    reddit = praw.Reddit(client_id='sQePkNsCdxJgOehVWkLa6A',\n",
    "                         client_secret='52ZjTdPwfQLP_RuXKcnDPe2Vs2Myxg',\n",
    "                         user_agent='<console:HAPPY:1.0')\n",
    "    subreddit = reddit.subreddit(search_term)\n",
    "    for submission in subreddit.hot(limit=post_limit):\n",
    "        for comment in submission.comments:\n",
    "            if hasattr(comment,'body'):\n",
    "                index = df.shape[0]\n",
    "                df.loc[index] = [submission.title, comment.body]\n",
    "    return df\n",
    "\n",
    "def cleanup_reddit(uncleaned_frame):\n",
    "    df = pd.DataFrame(columns=['Posts', 'Comments'])\n",
    "    uncleaned_frame.reset_index()\n",
    "    for index, row in uncleaned_frame.iterrows():\n",
    "        Post, Comment = row['Posts'], row['Comments']\n",
    "        if Comment == \"[deleted]\":\n",
    "            continue\n",
    "        subComments = []\n",
    "        commented = Comment.split(\". \")\n",
    "        for x in commented:\n",
    "            subComments.append(x)\n",
    "        for i in subComments:\n",
    "            commentina = re.split(\"[?:!]\", i)\n",
    "            for j in commentina:\n",
    "                j = clean(j, no_emoji=True)\n",
    "                if j == \"\":\n",
    "                    continue\n",
    "                index = df.shape[0]\n",
    "                df.loc[index] = [Post, j]\n",
    "    return df\n",
    "\n",
    "\n",
    "def combined_reddit(search_term: str, post_limit=10):\n",
    "    df = scrape_reddit(search_term,post_limit)\n",
    "    output = cleanup_reddit(df)\n",
    "    return output\n",
    "        \n",
    "dataframe_reddit = combined_reddit(\"dove shampoo\")\n",
    "save_data(dataframe_reddit, \"reddit_scrapped\")        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Youtube"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using youtube's API, we are able to find the top videos of a certain search and scrape the comments from them.\n",
    "\n",
    "It starts by importing required packages such as “googleapiclient.discovery”, “youtube_transcript_api”, “pandas”, “deepmultilingualpunctuation”, and “cleantext”. Then, it defines a function called “youtube_search” that takes two arguments: “search_term” and “maxresults”, which represent the search term and the maximum number of results to be returned.\n",
    "\n",
    "The function uses the YouTube Data API to search for videos that match the “search_term” and returns the video IDs for the top “maxresults” videos. It stores the video IDs in a list called “vidID”, which is returned by the function.\n",
    "\n",
    "The next function is “youtube_comments”, which takes a list of video IDs as the input and returns a pandas data frame that contains the title, comments, and likes for each comment in the videos. The function uses the YouTube Data API to get the title of each video and then uses the “commentThreads.list” method to get the comments and likes for each video.\n",
    "\n",
    "The comments and likes are extracted from the response and are cleaned using the clean method from “cleantext”.  If there are any replies to a comment, they are also extracted and added to the data frame.\n",
    "\n",
    "Finally, the script calls the “youtube_search” and “youtube_comments” functions with appropriate arguments and saves the resulting data frame using the “save_data” function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import pandas as pd\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "from cleantext import clean\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"Insert your own key here\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "\n",
    "def youtube_search(search_term, maxresults):  # returns video ID\n",
    "    # assuming that maxresults is always <=50\n",
    "    vidID = []\n",
    "\n",
    "    # print(\"Searching for videos\")\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        q=\" \".join(search_term),\n",
    "        part=\"id\",\n",
    "        type=\"video\",\n",
    "        maxResults=maxresults)\n",
    "\n",
    "    search_response = request.execute()\n",
    "\n",
    "    for i in range(maxresults):\n",
    "        videoID = search_response['items'][i]['id'][\"videoId\"]\n",
    "        # videoLinks = \"https://www.youtube.com/watch?v=\" + videoID\n",
    "        vidID.append(videoID)\n",
    "\n",
    "    return vidID\n",
    "\n",
    "\n",
    "# using video ID, get all the comments and likes to put into dataframe\n",
    "def youtube_comments(vidID):\n",
    "    df = pd.DataFrame(columns=['title', 'Comments', 'likes'])\n",
    "    for i in range(len(vidID)):\n",
    "\n",
    "        title, video_response = get_title(vidID[i])\n",
    "\n",
    "        try:  # use try/except to check if comments exists\n",
    "            comment_count = video_response['items'][0]['statistics']['commentCount']\n",
    "            # print(\"Video-\", title, \"-- Comment count: \", comment_count)\n",
    "\n",
    "            request_comment = youtube.commentThreads().list(\n",
    "                part=\"snippet, replies\",\n",
    "                videoId=vidID[i])\n",
    "\n",
    "            comment_response = request_comment.execute()\n",
    "            df = get_all_comments(comment_response, title, df)\n",
    "            test = comment_response.get(\"nextPageToken\", \"nil\")\n",
    "\n",
    "            while test != 'nil':  # load next page of comments\n",
    "                next_page_ = comment_response.get('nextPageToken')\n",
    "                request = youtube.commentThreads().list(  # new request for next pag\n",
    "                    part=\"snippet,replies\",\n",
    "                    pageToken=next_page_,\n",
    "                    videoId=vidID[i]\n",
    "                )\n",
    "                comment_response = request.execute()\n",
    "\n",
    "                df = get_all_comments(comment_response, title, df)\n",
    "                test = comment_response.get('nextPageToken', 'nil')\n",
    "\n",
    "        except:\n",
    "            print(\"Video\", i + 1, \"-\", title,\n",
    "                  \"-- Comments are turned off, ignoring video\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_title(vid_id):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet, statistics\",\n",
    "        id=vid_id)\n",
    "\n",
    "    video_response = request.execute()\n",
    "    title = video_response['items'][0]['snippet']['title']\n",
    "    return title, video_response\n",
    "\n",
    "\n",
    "def get_all_comments(response, title, df):\n",
    "    for comment in response['items']:\n",
    "        comment_text = comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comment_text = clean(comment_text, no_emoji=True)\n",
    "        likes_count = comment['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "        # print(comment_text, likes_count)\n",
    "        if 'replies' in comment.keys():\n",
    "            for reply in comment['replies']['comments']:\n",
    "                rtext = reply['snippet']['textDisplay']\n",
    "                rtext = clean(rtext, no_emoji=True)\n",
    "                rlike = reply['snippet']['likeCount']\n",
    "                # print(rtext, rlike)\n",
    "                df = df.append({\"title\": title, \"Comments\": rtext,\n",
    "                               \"likes\": rlike}, ignore_index=True)\n",
    "\n",
    "        df = df.append({\"title\": title, \"Comments\": comment_text,\n",
    "                       \"likes\": likes_count}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "search_terms = [product] + [\"review\"]\n",
    "vid_id = youtube_search(search_terms, 5)\n",
    "df_comments = youtube_comments(vid_id)\n",
    "save_data(df_comments, \"youtube comments\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scrapping all these websites, the following CSVs were saved into Data:\n",
    "\n",
    "“amazon_reviews.csv”: Each row contains the Title of the item, the Link to it, and the Stars and the review for each comment.\n",
    "\n",
    " “reddit_scrapped.csv”: Each row contains the Posts that the comment comes from, as well as the comment itself.\n",
    "\n",
    " “youtube_comments.csv”: Each row contains the title the comment comes from, the comment itself and the likes each comment received.\n",
    "\n",
    "\n",
    "All the data are currently very messy and have different headers for storing the same kind of information. The CSVs will be cleaned and stored in “First_Pass” after processing and standardisation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are processing all the information collected from the previous step so we have to take the following inputs from the previous steps:\n",
    "\n",
    "product: A string of the product used for analysis, this was generated before Pre-Analysis.\n",
    "\n",
    "competitors: A dictionary of competing products and their associated companies. This was generated by OpenAI during Pre-Analysis.\n",
    "\n",
    "CSVs of comments: Various CSVs containing the comments scrapped from various websites. This will be found in the root directory of the  Data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"Dove Shampoo\"\n",
    "\n",
    "competitors = {\n",
    "    \"Pantene\"\n",
    "    \"L' Oreal Paris\"\n",
    "    \"Head & Shoulders\"\n",
    "    \"Clear\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 First filter to remove unrelated comments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data collection, it is likely that a considerable number of comments may not provide relevant or useful information pertaining to the product. As a result, data cleaning is imperative in order to ensure proper categorization of comments.\n",
    "\n",
    "To classify the comments, a zero-shot classification technique was employed utilizing a pre-trained model. This method involves providing specific labels and analyzing the correlation between each comment and each tag.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Generating labels for zero-shot classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate data cleaning, it is necessary to have categories to sort unrelated comments into. The \"first_filter_keywords\" set comprises predetermined generic categories that aim to capture unrelated comments.\n",
    "\n",
    "To generate the labels required for data cleaning, the function \"get_first_filter(product, competitors, filters)\" takes as inputs the product, its competitors, and the predetermined filters. It combines these inputs to create a list of labels for the initial round of zero-shot classification.\n",
    "\n",
    "Next, we add a label for the product, along with its competing products, by using the competitor's dictionary generated using OpenAI in the preceding steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the full list of fake categories and actual categories for the product\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "    # file = open(file_path, encoding=\"utf8\")\n",
    "    # cleaned = list(csv.reader(file))[1:]\n",
    "    # return cleaned\n",
    "    \n",
    "\n",
    "first_filter_keywords = [\n",
    "        'first',\n",
    "        'game',\n",
    "        'app',\n",
    "        'Thanks',\n",
    "        'great video',\n",
    "        'links href a',\n",
    "        'subscribed',\n",
    "        'offtopic',\n",
    "    ]\n",
    "\n",
    "def get_first_filter(product, competitors, filters):\n",
    "    filter = []\n",
    "    filter.append(\"Comment about \" + product)\n",
    "    for i in competitors:\n",
    "        filter.append(\"Comment about \" + i)\n",
    "    filter = filter + filters\n",
    "    return filter\n",
    "\n",
    "\n",
    "\n",
    "first_filter = get_first_filter(product, competitors, first_filter_keywords)\n",
    "first_filter\n",
    "\n",
    "#So ummm afer cleaning 4400 comments, 1412 are more than 0.3 and only 464 are related lmao\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The is the first_filter generated for the case of the Dove Shampoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_filter = ['Comment about iphone',\n",
    " 'first',\n",
    " 'Thanks',\n",
    " 'links href a',\n",
    " 'video review',\n",
    " 'subscribed',\n",
    " 'offtopic']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Zero shot classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the labels we can run the zero-shot classifier on them.\n",
    "\n",
    "The code was run on Collab which allows the progress to be periodically saved and run in the background as other work is being done on the computer. There is a CSVsForAid  folder which is where one would put the CSV for processing and there is another Output folder inside the folder where the output of the code is deposited.\n",
    "\n",
    "“save_to_drive(df, filename)” - function is used to save the existing progress on the data frame to the drive, allowing for progress to be logged frequently in case collab crashes.\n",
    "\n",
    "“read_from_drive(file_path, specific_column)” - function is used to load uninitialised CSVs which have not been worked on before.\n",
    "\n",
    "“check_progress_csv(file_path)” - function is used to load CSVs with some progress in them which are already stored in the output folder on the drive.\n",
    "\n",
    "“df_labeller_by_20s(filename, factors, specific_column)” - function is used to label all the data in the CSV. It takes in the filename, and the first_filter as the factors and requires the specific_column where the comments are.\n",
    "\n",
    "It will output a CSV with the columns ‘Comments’:  the sentence,  ‘label’:  which category the sentence falls in, and ‘score’: how confident the model is in its prediction for that sentence.\n",
    "\n",
    "The function first checks if existing progress has been made by checking the Output folder with  “check_progress_csv(file_path)”. If no progress has been made, it will use  “read_from_drive(file_path, specific_column)” to get the raw CSV and initialise the data frame properly with the relevant columns as elaborated in the previous paragraph. Anything that has not been labeled will have a label and score set to 'Not Done' and an invalid sentence will have its label and score set to 'Not Valid Sentence'.\n",
    "\n",
    "The function then iterates through the entire input data frame and runs the classifier on each line and saves the CSV into the drive once it has made 20 predictions with more than 30% certainty.\n",
    "\n",
    "Finally, the function returns the entire data frame with the newly added classifications. The function is called at the end of the code with the specific parameters (\"youtube_comments\", first_filter, 'comments'), and the result is stored in the answer variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "\n",
    "\n",
    "def save_to_drive(df,filename):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/Output/' + filename + '_output.csv'\n",
    "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "    df.to_csv(f)\n",
    "\n",
    "def read_from_drive(file_path, specific_column):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/' + file_path + '.csv'\n",
    "  df = pd.read_csv(path)\n",
    "  answer = df[[specific_column]]\n",
    "  return answer\n",
    "\n",
    "def check_progress_csv(file_path):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/Output/' + file_path + '_output.csv'\n",
    "  try:\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "  except:\n",
    "    return 1\n",
    "  \n",
    "def df_labeller_by_20s(filename, factors, specific_column):\n",
    "  #Label every 20 sets and then save to the csv\n",
    "  #If df does not exist in proper form first then create\n",
    "  #Find the column to start\n",
    "  \n",
    "  #Open the csv from drive\n",
    "  trail = check_progress_csv(filename)\n",
    "  if isinstance(trail, pd.DataFrame):\n",
    "    print(\"Progress csv found, starting from existing index\")\n",
    "    df_input = check_progress_csv(filename)[[specific_column, 'label', 'score']]\n",
    "  else:\n",
    "    print(\"No progress csv found, reading from folder\")\n",
    "    df_input = read_from_drive(filename, specific_column)\n",
    "    #Initialise the dataframe and add the columns if not available\n",
    "    columns = df_input.columns\n",
    "    if 'label' not in columns:\n",
    "      df_input['label'] = 'Not Done'\n",
    "      df_input['score'] = 'Not Done'\n",
    "\n",
    "  #iterate through the whole df and every 100, save the information\n",
    "  #finding the first instance of Not Done\n",
    "  index = (df_input[df_input.label == \"Not Done\"].index[0])\n",
    "  ending_index = df_input.shape[0]\n",
    "  count = 0\n",
    "\n",
    "  #Proceed to iterate through\n",
    "  while index < ending_index:\n",
    "    if count == 20:\n",
    "      save_to_drive(df_input, filename)\n",
    "      count = 0\n",
    "      now = datetime.now()\n",
    "      dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "      print(\"saved \" + str(index) + \" sentence at \" + dt_string)\t\n",
    "      \n",
    "    try:\n",
    "        sentence = df_input.loc[index][-3]\n",
    "        result = classifier(sentence, factors)\n",
    "        label = result['labels'][0]\n",
    "        score = result['scores'][0]\n",
    "        df_input.loc[index]['label'] = label\n",
    "        df_input.loc[index]['score'] = score\n",
    "        if score >= 0.3:\n",
    "            count += 1\n",
    "            index += 1\n",
    "    except:\n",
    "      df_input.loc[index]['label'] = 'Not Valid Sentence'\n",
    "      df_input.loc[index]['score'] = 'Not Valid Sentence'\n",
    "      print('skipping' + str(index))\n",
    "      index += 1\n",
    "  #Leave here and save one last time to confirm\n",
    "  #function to save\n",
    "  save_to_drive(df_input, filename)\n",
    "  return df_input\n",
    "\n",
    "\n",
    "answer = df_labeller_by_20s(\"youtube_comments\", first_filter, 'comments')\n",
    "answer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the processing was done on the selected the CSVs they were downloaded and place back into this folder under <b> First_pass </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Recombining all the data together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was then scanned through and the sentences that make it through to the second round of classification are selected. Only comments related to the product and whose confidence in classification is more than 30% will be selected for evaluation.\n",
    "\n",
    "The code defines a function “combine_csvs(folder_location, product)”. It loops through each file in the specified folder location and for each row in the CSV, it checks if the score is above a specified threshold of 0.3 and if the label matches the specified product filter. If so, it appends the comment to pandas Data Frame “product_stuff”. Finally, the function saves the “product_stuff” Data Frame to a new CSV file with a specified name using “save_data” function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In between the filtering: combine those related to dove shampoo together and those to the competitors together, dropping the others (only added if more than 0.3 certain between layers)\n",
    "#Standardised all other scrappers to have Comments, label, score tabs\n",
    "import pandas as pd\n",
    "import os\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def combine_csvs(folder_location, product):\n",
    "    product_stuff = pd.DataFrame(columns=['Comments'])\n",
    "    product_filter = \"Comment about \" + product\n",
    "    certainty = 0.3\n",
    "    for filename in os.listdir(folder_location):\n",
    "        print(filename)\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(folder_location, filename)\n",
    "            dataframe = read_csv(filepath) \n",
    "            for index, row in dataframe.iterrows():\n",
    "                comment = str(row['Comments']).strip()\n",
    "                label = row['label']\n",
    "                score = row['score']\n",
    "                if score ==  \"Not Valid Sentence\":\n",
    "                    continue\n",
    "                if score == \"Not Done\":\n",
    "                    break\n",
    "                if float(score) > certainty:\n",
    "                    if label == product_filter:\n",
    "                        product_stuff.loc[len(product_stuff)]  = {\"Comments\": comment}  \n",
    "    return product_stuff\n",
    "    \n",
    "                       \n",
    "this =combine_csvs('Data/First_pass', 'Dove Shampoo', competitors)     \n",
    "save_data(this, \"Second_pass/check\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function “save_data(data, file_name)” will then take the data frame returned from  “combine_csvs(folder_location, product)”,  and will then store it in the “Second_pass” folder with the file_name: check.csv."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Classification back into categories identified in part 2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, it can be put into the categories based on the <b> keywords </b> identified earlier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Labels used for round 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the keywords found in Pre-Analysis to serve as the categories to sort the data. As they could still be unrelated comments, a second set of generic filters was created to filter out unwanted comments.\n",
    "\n",
    "The “get_categorization_filter()” function takes in two lists of strings - keywords and filters and returns a combined list of strings that represents the filters to be applied to comments. The categories variable is created by calling the “get_categorization_filter()” function with keywords and “second_filter_keywords” as arguments. It is a list of strings that includes the keywords list and some additional generic filter keywords. These filter keywords will be used to filter out comments that are not relevant to the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorization_filter(keywords, filters):\n",
    "    final_filter = keywords + filters\n",
    "    return final_filter\n",
    "\n",
    "#might need a second filter ? using first_filter_keywords as a stand in\n",
    "#keywords come from create_keywords in 2c\n",
    "\n",
    "second_filter_keywords = [\n",
    "    'generic',\n",
    "    'popularity',\n",
    "    'links',\n",
    "    'offtopic',\n",
    "    'suggestion',\n",
    "    'advice'\n",
    "]\n",
    "\n",
    "\n",
    "categories = get_categorization_filter(keywords, second_filter_keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the output for the 'Dove Shampoo':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Zero shot classification round 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code functions the same as the previous zero-shot classification, however, the factors used have changed from “first_filter”  to “categories”. The output of the code will now include the labels and scores assigned to each input sentence based on the \"categories\" factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "\n",
    "\n",
    "def save_to_drive(df,filename):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/Output/' + filename + '_output.csv'\n",
    "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "    df.to_csv(f)\n",
    "\n",
    "def read_from_drive(file_path, specific_column):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/' + file_path + '.csv'\n",
    "  df = pd.read_csv(path)\n",
    "  answer = df[[specific_column]]\n",
    "  return answer\n",
    "\n",
    "def check_progress_csv(file_path):\n",
    "  path = '/content/drive/My Drive/CSVsForAid/Output/' + file_path + '_output.csv'\n",
    "  try:\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "  except:\n",
    "    return 1\n",
    "  \n",
    "def df_labeller_by_20s(filename, factors, specific_column):\n",
    "  #Label every 20 sets and then save to the csv\n",
    "  #If df does not exist in proper form first then create\n",
    "  #Find the column to start\n",
    "  \n",
    "  #Open the csv from drive\n",
    "  trail = check_progress_csv(filename)\n",
    "  if isinstance(trail, pd.DataFrame):\n",
    "    print(\"Progress csv found, starting from existing index\")\n",
    "    df_input = check_progress_csv(filename)[[specific_column, 'label', 'score']]\n",
    "  else:\n",
    "    print(\"No progress csv found, reading from folder\")\n",
    "    df_input = read_from_drive(filename, specific_column)\n",
    "    #Initialise the dataframe and add the columns if not available\n",
    "    columns = df_input.columns\n",
    "    if 'label' not in columns:\n",
    "      df_input['label'] = 'Not Done'\n",
    "      df_input['score'] = 'Not Done'\n",
    "\n",
    "  #iterate through the whole df and every 100, save the information\n",
    "  #finding the first instance of Not Done\n",
    "  index = (df_input[df_input.label == \"Not Done\"].index[0])\n",
    "  ending_index = df_input.shape[0]\n",
    "  count = 0\n",
    "\n",
    "  #Proceed to iterate through\n",
    "  while index < ending_index:\n",
    "    if count == 20:\n",
    "      save_to_drive(df_input, filename)\n",
    "      count = 0\n",
    "      now = datetime.now()\n",
    "      dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "      print(\"saved \" + str(index) + \" sentence at \" + dt_string)\t\n",
    "      \n",
    "    try:\n",
    "        sentence = df_input.loc[index][-3]\n",
    "        result = classifier(sentence, factors)\n",
    "        label = result['labels'][0]\n",
    "        score = result['scores'][0]\n",
    "        df_input.loc[index]['label'] = label\n",
    "        df_input.loc[index]['score'] = score\n",
    "        if score >= 0.3:\n",
    "            count += 1\n",
    "            index += 1\n",
    "    except:\n",
    "      df_input.loc[index]['label'] = 'Not Valid Sentence'\n",
    "      df_input.loc[index]['score'] = 'Not Valid Sentence'\n",
    "      print('skipping' + str(index))\n",
    "      index += 1\n",
    "  #Leave here and save one last time to confirm\n",
    "  #function to save\n",
    "  save_to_drive(df_input, filename)\n",
    "  return df_input\n",
    "\n",
    "\n",
    "answer = df_labeller_by_20s(\"youtube_comments\", categories, 'comments')\n",
    "answer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing, all the data, the comments that will be used to derive our design opportunities for the product have finally been separated out. \n",
    "\n",
    "<b> check_output.csv </b>: This is the csv holding all the data, it has the comment, the label for what categories it belongs to and the confidence score. It is located in the <b> Data/Final</b> folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Identification of areas of opportunity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, take all the information collected from the previous steps and derive the outputs.\n",
    "\n",
    "product: A string of the product used for analysis, this was generated before Pre-Analysis.\n",
    "\n",
    "keywords: a list of keywords generated as the output of Pre-Analysis, this will be used as the categories to filter the comments into.\n",
    "\n",
    "competitors: A dictionary of competing products and their associated companies. This was generated by openAI in Pre-Analysis.\n",
    "CSVs of final comments: This is the CSV of the comments that we will be analysing. It can be found in the  Data/Final folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'Dove shampoo'\n",
    "\n",
    "keywords = [\"Hair type\", \"Ingredients\", \"Brand Reputation\", \"Gentle formula\", \"Moisturizing properties\", \"Product claims\"]\n",
    "\n",
    "competitors = {\n",
    "    \"Pantene\": \"Pantene hair\",\n",
    "    \"L' Oreal Paris\": \"L'Oreal Paris hair\",\n",
    "    \"Head & Shoulders\": \"Head & Shoulders hair\",\n",
    "    \"Clear\": \"Clear hair\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.1 Intended Outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The present study aims to deliver three key outputs that can aid in improving the product under consideration:\n",
    "\n",
    "Firstly, we will identify the top three categories that future iterations of the product should focus on. This will be determined by conducting a quantitative analysis of the data obtained. The categories with the highest absolute score will be considered the most significant and will be returned as the top three categories that should be prioritized. \n",
    "\n",
    "Secondly, we will explore the ways to enhance the top three categories that future iterations of the product should concentrate on. This will be accomplished by performing a qualitative analysis of all comments pertaining to the top categories, grouping them together, and summarizing the findings to determine the best approach to improving the category. \n",
    "\n",
    "Finally, we will provide a design problem statement that can be used to initiate ideation on improving the product. OpenAI will be used to generate a design prompt based on the top three categories identified in output 1. This will serve as a starting point for designers to brainstorm ideas to improve the product.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Sentiment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code performs sentiment analysis on a dataset of comments using the Hugging Face Transformers library. The goal of the function “sentiment_labeller” is to label each comment as positive or negative and store this sentiment along with the corresponding category of the comment. The function takes a dataframe “df” and a list of categories as inputs. The “df” is the dataframe containing the comments to be labelled and categories is a list of processed keywords that will be used to label the comments.\n",
    "\n",
    "The function first creates a pipeline for sentiment analysis using the pipeline function from the Transformers library. It then iterates over each row in the “df” and extracts the comment and its corresponding category. It stores all the comments in a list called sentences.\n",
    "\n",
    "Next, the function uses the sentiment pipeline to label each comment as either positive or negative and stores this in a new column called “Sentiment” in the “df”. It then creates three empty dictionaries: “storage”, “positive_word_storage”, and “negative_word_storage”. “storage” is used to store the scores for each category, while “positive_word_storage” and “negative_word_storage” are used to store the positive and negative comments for each category, respectively.\n",
    "\n",
    "The function then iterates over each row in the “df” again, extracts the comment, category, and sentiment, and uses them to update the appropriate dictionaries. If the comment's category is in the categories list, the function updates the storage dictionary with the sentiment score for that category. If the sentiment is positive, the function appends the comment to the “positive_word_storage” dictionary for that category, otherwise it appends it to the “negative_word_storage” dictionary. The function also appends a period to the end of the comment if it doesn't already end in a punctuation mark.\n",
    "\n",
    "Finally, the function sorts the storage dictionary by the absolute value of the scores in descending order and returns it as a list along with the “positive_word_storage” and “negative_word_storage” dictionaries.\n",
    "\n",
    "The returned output consists of three main parts: Qualitative, positive, and negative. Qualitative is a list of tuples containing the categories sorted by the absolute values of their scores. The positive and negative dictionaries contain the positive and negative comments for each category, respectively. These outputs can be used to determine the top three categories that future iterations of the product should focus on, as well as ways to improve these categories and a design problem statement to start designers ideating on improving the product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "#Sentiment analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "processed = read_csv('Data\\Final\\check_output.csv')\n",
    "\n",
    "\n",
    "#The goal of this function is to take a dataset and for each comment in it label it is something positive or negative, the data would then be send to the word storages \n",
    "#df is the dataframe to be labelled and categories is the processed keywords\n",
    "#categories\n",
    "def sentiment_labeller(df, categories):\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    sentences = []\n",
    "    for index, row in df.iterrows():\n",
    "        sentence, category = row['Comments'], row['label']\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    #Takes the labels positive and negative and adds it as a new column to the dataframe supplied\n",
    "    new_list = list(map(lambda x: x['label'], sentiment_pipeline(sentences)))\n",
    "    df['Sentiment'] = new_list\n",
    "    \n",
    "    #Now create the stuff to store the things in\n",
    "    #postive_word_storage is the compliments in the category\n",
    "    #negative_word_storage is the complains in the category\n",
    "    #storage is just the score\n",
    "    storage = {}\n",
    "    positive_word_storage = {}\n",
    "    negative_word_storage = {}\n",
    "    for x in categories:\n",
    "        storage[x] = 0\n",
    "        positive_word_storage[x] = \"\"\n",
    "        negative_word_storage[x] = \"\"\n",
    "        \n",
    "    #iterate through all the rows\n",
    "    for index, row in df.iterrows():\n",
    "        comment = row['Comments']\n",
    "        category = row['label']\n",
    "        positiveNegative = row['Sentiment']\n",
    "        active = None\n",
    "        if category in categories:\n",
    "            if positiveNegative == \"POSITIVE\":\n",
    "                storage[category] += 1\n",
    "                active = positive_word_storage\n",
    "            else:\n",
    "                storage[category] -= 1\n",
    "                active = negative_word_storage\n",
    "            comment = comment.strip()\n",
    "            if comment[-1] in string.punctuation:\n",
    "                active[category] = active[category] + comment\n",
    "            else:\n",
    "                active[category] = active[category] + comment + '.'\n",
    "        Qualatative = sorted(list(storage.items()), key= lambda x: abs(x[1]), reverse=True)\n",
    "    return Qualatative, positive_word_storage, negative_word_storage\n",
    "#Quantitative data analysed, now go find the largest numerical category and if they are good or bad\n",
    "\n",
    "Qualatative, positive, negative = sentiment_labeller(processed,keywords)\n",
    "\n",
    "#Done once for the dove shampoo and once for the competition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Getting suggestions for improvement based on comments and design prompt (qualitative analysis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function “get_best_way_to_improve_quality” takes in the “points” and “extracted_comments” obtained from the “get_outputs” function, and the product name (Dove Shampoo), and then iterates through the list of points. For each point, it prompts the GPT model with a question that asks for the best way to improve that aspect of the product based on the extracted comments. It then passes this prompt to the “ask_gpt” function which generates a response using the OpenAI GPT-3 language model. The function stores each response in a “dictionary ways_to_improve” with the corresponding aspect as the key. Finally, it returns the dictionary “ways_to_improve”.\n",
    "\n",
    "\n",
    "The “get_design_problem_statement” function takes in the product name (Dove Shampoo) and the top 3 qualities to focus on, and uses GPT-3 to generate a design problem statement to improve the product centered around those qualities. The function constructs a prompt string with the given inputs, and then uses the “ask_gpt” function to again generate a response from GPT-3. Finally, the function returns the response string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function then takes the points and extracted comments and ask chatgpt to summarise the best way to improve in the various categories\n",
    "def get_best_way_to_improve_quality(points, extracted_comments, product):\n",
    "    ways_to_improve = {}\n",
    "    for i in range(len(points)):\n",
    "        prompt = \"According to the extracted comments, what is the best way for the company of the \" + product + \" to improve following aspect of the \"+ product  + \"? Aspect: \" + points[i] + \". Extracted Comments: \" + extracted_comments[i]\n",
    "        response = ask_gpt(prompt)\n",
    "        print(response)\n",
    "        ways_to_improve[points[i]] = response\n",
    "    return ways_to_improve\n",
    "\n",
    "responsed = get_best_way_to_improve_quality(x,y, product)\n",
    "print(responsed)\n",
    "\n",
    "#This function will take the points and use openai to create a design prompt for the product using the points extracted previously\n",
    "def get_design_problem_statement(product, points):\n",
    "    pointers = \"\"\n",
    "    for i in points:\n",
    "        pointers = pointers + i\n",
    "    prompt = \"Create a Design problem statement to improve the \" + product + \" centering around the following qualities: \" + pointers\n",
    "    return ask_gpt(prompt)\n",
    "    \n",
    "response = get_design_problem_statement(product, x)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Outputs and reflections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code writes the output generated from the previous functions to a file named “output.txt”. It first opens the file in write mode using the 'with open' statement and writes a header line indicating the product name.\n",
    "\n",
    "Then it writes the weakest areas of the product and categories to improve based on the comments analyzed.\n",
    "\n",
    "Next, it writes the suggestions for improving the product's weakest areas as generated by the “get_best_way_to_improve_quality” function.\n",
    "Finally, it writes the design problem statement generated by the “get_design_problem_statement” function. Each output section is separated by two newline characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    f.write('This is the output for the product: ')  \n",
    "    f.write(product+ '\\n\\n\\n') \n",
    "    \n",
    "    f.write(\"Output one: Product's weakest areas\" + '\\n')\n",
    "    f.write('According to the comments analysed, the product is weakest in the following categories: ' + '\\n\\n')\n",
    "    \n",
    "    \n",
    "    categories_to_improve = \"\"\n",
    "    for i in range(len(x)):\n",
    "        categories_to_improve = categories_to_improve + x[i]\n",
    "        if i != len(x) - 1:\n",
    "            categories_to_improve = categories_to_improve + ', '\n",
    "            \n",
    "    f.write(categories_to_improve + '\\n\\n')\n",
    "    \n",
    "    f.write('\\n\\n')\n",
    "\n",
    "    \n",
    "    f.write(\"Output two: Suggested means to improve the product's weakest areas:\" + '\\n')\n",
    "    f.write('These are the suggestions for a company to improve in the following categories: ' + '\\n\\n')\n",
    "    for j in responsed:\n",
    "        f.write(j + \" : \" + responsed[j] + '\\n\\n' )\n",
    "        \n",
    "    f.write('\\n\\n')\n",
    "\n",
    "    f.write(\"Output three: A design problem statement to start designers ideating on improve the product:\" + '\\n')\n",
    "    f.write(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output 1: Product's weakest areas According to the comments analysed.\n",
    "\n",
    "Output 2: Suggested means to improve the product's weakest areas.\n",
    "\n",
    "Output 3: A design problem statement to start designers ideating on improving the product.\n",
    "\n",
    "Output 4: A set of A.I. generated images to aid designers in visualizing potential improvements: By training an A.I. model on the data gathered from the qualitative analysis of customer comments, we can generate a set of images that showcase potential design improvements. This can help designers to visualize how the product could be improved based on customer feedback, and can aid in the ideation and design process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Analysis and Reflection\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Taken to Run: The time taken for each web scraper and code to run is significant. While the use of collab and several computers running code simultaneously allowed for the process to be completed in a somewhat timely manner (36 hours across 2 computers for 72 hours (both running collab and one running vsc as well)). \n",
    "\n",
    "Dataset Size: Inspecting the CSVs show that even though a valid output was produced not all the data was labeled and some platforms provided a much larger dataset than others. While this could be improved by running the code for a longer time, it would come at the expense of time.\n",
    "\n",
    "Limitation of Crowdsourcing: Although the information has all been collected from general platforms, the method of getting the general consensus may not be 100% correct. Users may not always know what they want. \n",
    "\n",
    "Lack of Regional Information and User Base: The code does not classify and sort data by who the users are or based on regional information which could be critical for certain products. This could make it hard to target specific problems that a specific user base has. \n",
    "\n",
    "Detection of similar word forms in keywords: While the code identifies how similar a keyword is to the rest, it may not be able to recognise that the words are similar if they are in different forms (e.g. durable vs durability, charges quickly vs charging speed). A human could manually sort through these for more accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
